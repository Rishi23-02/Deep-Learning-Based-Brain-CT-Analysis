{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e2b89a-175d-4613-83fd-03807bf0251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: SimpleITK in /opt/venv/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd453a8a-b9c3-48bb-86ea-12a2cea452b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V10_PERCEPTUAL_LOSS\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET50\n",
      "  - Loss: L1 (weight=1.0) + Perceptual (weight=0.1)\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 548M/548M [00:00<00:00, 628MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perceptual Loss info:\n",
      "  - Feature extractor: VGG19 (pretrained, frozen)\n",
      "  - Feature layers: relu1_2, relu2_2, relu3_2\n",
      "  - Grayscale handling: replicate to 3 channels before VGG\n",
      "  - Total loss = 1.0 * L1 + 0.1 * Perceptual\n",
      "  - Expected: Better structural/texture preservation in SR outputs\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Perceptual Loss\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [00:43<00:00, 30.83it/s, loss=0.000637, l1=0.000532, perc=0.001051]\n",
      "Epoch 2/50: 100% 1328/1328 [00:41<00:00, 32.20it/s, loss=0.008034, l1=0.004197, perc=0.038374]\n",
      "Epoch 3/50: 100% 1328/1328 [00:38<00:00, 34.37it/s, loss=0.014466, l1=0.007409, perc=0.070563]\n",
      "Epoch 4/50: 100% 1328/1328 [00:38<00:00, 34.37it/s, loss=0.041659, l1=0.023722, perc=0.179379]\n",
      "Epoch 5/50: 100% 1328/1328 [00:38<00:00, 34.47it/s, loss=0.030805, l1=0.016514, perc=0.142912]\n",
      "Epoch 6/50: 100% 1328/1328 [00:37<00:00, 35.13it/s, loss=0.001494, l1=0.000706, perc=0.007879]\n",
      "Epoch 7/50: 100% 1328/1328 [00:38<00:00, 34.65it/s, loss=0.006900, l1=0.003429, perc=0.034709]\n",
      "Epoch 8/50: 100% 1328/1328 [00:38<00:00, 34.17it/s, loss=0.019197, l1=0.010547, perc=0.086500]\n",
      "Epoch 9/50: 100% 1328/1328 [00:38<00:00, 34.22it/s, loss=0.015633, l1=0.008664, perc=0.069695]\n",
      "Epoch 10/50: 100% 1328/1328 [00:39<00:00, 33.78it/s, loss=0.037853, l1=0.022757, perc=0.150962]\n",
      "Epoch 11/50: 100% 1328/1328 [00:39<00:00, 33.99it/s, loss=0.009833, l1=0.005143, perc=0.046901]\n",
      "Epoch 12/50: 100% 1328/1328 [00:39<00:00, 33.93it/s, loss=0.005736, l1=0.003071, perc=0.026651]\n",
      "Epoch 13/50: 100% 1328/1328 [00:39<00:00, 33.84it/s, loss=0.007375, l1=0.004500, perc=0.028750]\n",
      "Epoch 14/50: 100% 1328/1328 [00:38<00:00, 34.15it/s, loss=0.028262, l1=0.015217, perc=0.130449]\n",
      "Epoch 15/50: 100% 1328/1328 [00:38<00:00, 34.54it/s, loss=0.013479, l1=0.007428, perc=0.060508]\n",
      "Epoch 16/50: 100% 1328/1328 [00:38<00:00, 34.17it/s, loss=0.033974, l1=0.017854, perc=0.161194]\n",
      "Epoch 17/50: 100% 1328/1328 [00:39<00:00, 34.02it/s, loss=0.000191, l1=0.000168, perc=0.000226]\n",
      "Epoch 18/50: 100% 1328/1328 [00:37<00:00, 35.17it/s, loss=0.013438, l1=0.007070, perc=0.063679]\n",
      "Epoch 19/50: 100% 1328/1328 [00:37<00:00, 35.02it/s, loss=0.016605, l1=0.008983, perc=0.076213]\n",
      "Epoch 20/50: 100% 1328/1328 [00:39<00:00, 33.71it/s, loss=0.001532, l1=0.000884, perc=0.006478]\n",
      "Epoch 21/50: 100% 1328/1328 [00:39<00:00, 33.91it/s, loss=0.046955, l1=0.025404, perc=0.215507]\n",
      "Epoch 22/50: 100% 1328/1328 [00:38<00:00, 34.10it/s, loss=0.027922, l1=0.015368, perc=0.125542]\n",
      "Epoch 23/50: 100% 1328/1328 [00:39<00:00, 33.57it/s, loss=0.026587, l1=0.014947, perc=0.116397]\n",
      "Epoch 24/50: 100% 1328/1328 [00:39<00:00, 34.02it/s, loss=0.010996, l1=0.005632, perc=0.053634]\n",
      "Epoch 25/50: 100% 1328/1328 [00:38<00:00, 34.21it/s, loss=0.029974, l1=0.016206, perc=0.137677]\n",
      "Epoch 26/50: 100% 1328/1328 [00:38<00:00, 34.23it/s, loss=0.012699, l1=0.006681, perc=0.060184]\n",
      "Epoch 27/50: 100% 1328/1328 [00:38<00:00, 34.31it/s, loss=0.000147, l1=0.000131, perc=0.000168]\n",
      "Epoch 28/50: 100% 1328/1328 [00:39<00:00, 33.60it/s, loss=0.008177, l1=0.004675, perc=0.035028]\n",
      "Epoch 29/50: 100% 1328/1328 [00:39<00:00, 33.41it/s, loss=0.014559, l1=0.007690, perc=0.068696]\n",
      "Epoch 30/50: 100% 1328/1328 [00:39<00:00, 33.58it/s, loss=0.022362, l1=0.013233, perc=0.091289]\n",
      "Epoch 31/50: 100% 1328/1328 [00:39<00:00, 33.86it/s, loss=0.040486, l1=0.022160, perc=0.183255]\n",
      "Epoch 32/50: 100% 1328/1328 [00:38<00:00, 34.15it/s, loss=0.021817, l1=0.011495, perc=0.103216]\n",
      "Epoch 33/50: 100% 1328/1328 [00:38<00:00, 34.52it/s, loss=0.065481, l1=0.036859, perc=0.286222]\n",
      "Epoch 34/50: 100% 1328/1328 [00:39<00:00, 33.83it/s, loss=0.047077, l1=0.027600, perc=0.194777]\n",
      "Epoch 35/50: 100% 1328/1328 [00:38<00:00, 34.40it/s, loss=0.015373, l1=0.008520, perc=0.068530]\n",
      "Epoch 36/50: 100% 1328/1328 [00:39<00:00, 33.38it/s, loss=0.000256, l1=0.000115, perc=0.001418]\n",
      "Epoch 37/50: 100% 1328/1328 [00:39<00:00, 33.81it/s, loss=0.001297, l1=0.000521, perc=0.007765]\n",
      "Epoch 38/50: 100% 1328/1328 [00:38<00:00, 34.35it/s, loss=0.017557, l1=0.009427, perc=0.081303]\n",
      "Epoch 39/50: 100% 1328/1328 [00:39<00:00, 33.75it/s, loss=0.014678, l1=0.008498, perc=0.061802]\n",
      "Epoch 40/50: 100% 1328/1328 [00:39<00:00, 33.95it/s, loss=0.012235, l1=0.006583, perc=0.056518]\n",
      "Epoch 41/50: 100% 1328/1328 [00:39<00:00, 33.77it/s, loss=0.008418, l1=0.004380, perc=0.040372]\n",
      "Epoch 42/50: 100% 1328/1328 [00:39<00:00, 34.00it/s, loss=0.000069, l1=0.000059, perc=0.000096]\n",
      "Epoch 43/50: 100% 1328/1328 [00:38<00:00, 34.38it/s, loss=0.017855, l1=0.009898, perc=0.079574]\n",
      "Epoch 44/50: 100% 1328/1328 [00:42<00:00, 31.02it/s, loss=0.000030, l1=0.000019, perc=0.000110]\n",
      "Epoch 45/50: 100% 1328/1328 [00:48<00:00, 27.18it/s, loss=0.014089, l1=0.007185, perc=0.069047]\n",
      "Epoch 46/50: 100% 1328/1328 [00:49<00:00, 27.00it/s, loss=0.015196, l1=0.007989, perc=0.072068]\n",
      "Epoch 47/50: 100% 1328/1328 [00:48<00:00, 27.25it/s, loss=0.019686, l1=0.011029, perc=0.086576]\n",
      "Epoch 48/50: 100% 1328/1328 [00:49<00:00, 26.98it/s, loss=0.011637, l1=0.006305, perc=0.053327]\n",
      "Epoch 49/50: 100% 1328/1328 [00:49<00:00, 26.70it/s, loss=0.041880, l1=0.022319, perc=0.195616]\n",
      "Epoch 50/50: 100% 1328/1328 [00:50<00:00, 26.24it/s, loss=0.016218, l1=0.008763, perc=0.074548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.017746)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Perceptual Loss\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [01:45<00:00, 12.53it/s, loss=0.001950, l1=0.001084, perc=0.008654]\n",
      "Epoch 2/50: 100% 1328/1328 [01:47<00:00, 12.33it/s, loss=0.001090, l1=0.000567, perc=0.005233]\n",
      "Epoch 3/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.006807, l1=0.003929, perc=0.028781]\n",
      "Epoch 4/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.011897, l1=0.006469, perc=0.054286]\n",
      "Epoch 5/50: 100% 1328/1328 [01:57<00:00, 11.35it/s, loss=0.013449, l1=0.007372, perc=0.060774]\n",
      "Epoch 6/50: 100% 1328/1328 [01:57<00:00, 11.33it/s, loss=0.008577, l1=0.004660, perc=0.039176]\n",
      "Epoch 7/50: 100% 1328/1328 [01:57<00:00, 11.35it/s, loss=0.006768, l1=0.003887, perc=0.028811]\n",
      "Epoch 8/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.000887, l1=0.000490, perc=0.003969]\n",
      "Epoch 9/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.000537, l1=0.000290, perc=0.002470]\n",
      "Epoch 10/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.005731, l1=0.003134, perc=0.025969]\n",
      "Epoch 11/50: 100% 1328/1328 [01:57<00:00, 11.31it/s, loss=0.009660, l1=0.005090, perc=0.045704]\n",
      "Epoch 12/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.004606, l1=0.002527, perc=0.020799]\n",
      "Epoch 13/50: 100% 1328/1328 [01:57<00:00, 11.35it/s, loss=0.007859, l1=0.004087, perc=0.037719]\n",
      "Epoch 14/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.006098, l1=0.003298, perc=0.028003]\n",
      "Epoch 15/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.009629, l1=0.005352, perc=0.042770]\n",
      "Epoch 16/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.006959, l1=0.003695, perc=0.032639]\n",
      "Epoch 17/50: 100% 1328/1328 [01:57<00:00, 11.31it/s, loss=0.004852, l1=0.002679, perc=0.021731]\n",
      "Epoch 18/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.007724, l1=0.004290, perc=0.034333]\n",
      "Epoch 19/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.001748, l1=0.000908, perc=0.008399]\n",
      "Epoch 20/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.010664, l1=0.005585, perc=0.050799]\n",
      "Epoch 21/50: 100% 1328/1328 [01:57<00:00, 11.31it/s, loss=0.010410, l1=0.005631, perc=0.047790]\n",
      "Epoch 22/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.000632, l1=0.000282, perc=0.003504]\n",
      "Epoch 23/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.000198, l1=0.000109, perc=0.000886]\n",
      "Epoch 24/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.000951, l1=0.000468, perc=0.004834]\n",
      "Epoch 25/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.002537, l1=0.001442, perc=0.010954]\n",
      "Epoch 26/50: 100% 1328/1328 [01:57<00:00, 11.33it/s, loss=0.003876, l1=0.002084, perc=0.017916]\n",
      "Epoch 27/50: 100% 1328/1328 [01:57<00:00, 11.33it/s, loss=0.010252, l1=0.005446, perc=0.048059]\n",
      "Epoch 28/50: 100% 1328/1328 [01:57<00:00, 11.31it/s, loss=0.000160, l1=0.000087, perc=0.000725]\n",
      "Epoch 29/50: 100% 1328/1328 [01:57<00:00, 11.30it/s, loss=0.002911, l1=0.001703, perc=0.012074]\n",
      "Epoch 30/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.004922, l1=0.002616, perc=0.023054]\n",
      "Epoch 31/50: 100% 1328/1328 [01:57<00:00, 11.32it/s, loss=0.007677, l1=0.004325, perc=0.033522]\n",
      "Epoch 32/50: 100% 1328/1328 [01:57<00:00, 11.31it/s, loss=0.006553, l1=0.003470, perc=0.030831]\n",
      "Epoch 33/50: 100% 1328/1328 [01:57<00:00, 11.33it/s, loss=0.008888, l1=0.004974, perc=0.039139]\n",
      "Epoch 34/50: 100% 1328/1328 [01:57<00:00, 11.33it/s, loss=0.003909, l1=0.002122, perc=0.017868]\n",
      "Epoch 35/50: 100% 1328/1328 [01:57<00:00, 11.34it/s, loss=0.005888, l1=0.003240, perc=0.026485]\n",
      "Epoch 36/50: 100% 1328/1328 [01:54<00:00, 11.59it/s, loss=0.006581, l1=0.003555, perc=0.030262]\n",
      "Epoch 37/50: 100% 1328/1328 [01:49<00:00, 12.08it/s, loss=0.004721, l1=0.002556, perc=0.021649]\n",
      "Epoch 38/50: 100% 1328/1328 [01:53<00:00, 11.72it/s, loss=0.005529, l1=0.002966, perc=0.025631]\n",
      "Epoch 39/50: 100% 1328/1328 [01:54<00:00, 11.60it/s, loss=0.006343, l1=0.003400, perc=0.029435]\n",
      "Epoch 40/50: 100% 1328/1328 [01:54<00:00, 11.57it/s, loss=0.002785, l1=0.001501, perc=0.012835]\n",
      "Epoch 41/50: 100% 1328/1328 [01:52<00:00, 11.86it/s, loss=0.004862, l1=0.002723, perc=0.021391]\n",
      "Epoch 42/50: 100% 1328/1328 [01:39<00:00, 13.29it/s, loss=0.006457, l1=0.003460, perc=0.029971]\n",
      "Epoch 43/50: 100% 1328/1328 [01:38<00:00, 13.53it/s, loss=0.003896, l1=0.001984, perc=0.019123]\n",
      "Epoch 44/50: 100% 1328/1328 [01:38<00:00, 13.47it/s, loss=0.001511, l1=0.000816, perc=0.006954]\n",
      "Epoch 45/50: 100% 1328/1328 [01:30<00:00, 14.69it/s, loss=0.002441, l1=0.001272, perc=0.011693]\n",
      "Epoch 46/50: 100% 1328/1328 [01:25<00:00, 15.46it/s, loss=0.000095, l1=0.000085, perc=0.000098]\n",
      "Epoch 47/50: 100% 1328/1328 [01:20<00:00, 16.41it/s, loss=0.000081, l1=0.000077, perc=0.000041]\n",
      "Epoch 48/50: 100% 1328/1328 [01:20<00:00, 16.46it/s, loss=0.006087, l1=0.003374, perc=0.027130]\n",
      "Epoch 49/50: 100% 1328/1328 [01:20<00:00, 16.45it/s, loss=0.006020, l1=0.003105, perc=0.029152]\n",
      "Epoch 50/50: 100% 1328/1328 [01:21<00:00, 16.38it/s, loss=0.004814, l1=0.002543, perc=0.022711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.004469)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0% 0/332 [00:00<?, ?it/s]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30:  99% 330/332 [00:26<00:00, 17.16it/s, acc=78.80%]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 332/332 [00:27<00:00, 12.29it/s, acc=78.79%]\n",
      "Epoch 2/30: 100% 332/332 [00:19<00:00, 16.88it/s, acc=87.36%]\n",
      "Epoch 3/30: 100% 332/332 [00:19<00:00, 16.75it/s, acc=91.28%]\n",
      "Epoch 4/30: 100% 332/332 [00:19<00:00, 16.78it/s, acc=93.43%]\n",
      "Epoch 5/30:  60% 198/332 [00:11<00:07, 17.45it/s, acc=94.51%]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v10_perceptual_loss (Perceptual Loss)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | Perceptual Loss\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v10_perceptual_loss'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v10'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v10_perceptual_loss specific - USING PERCEPTUAL LOSS\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    # Loss weights\n",
    "    L1_WEIGHT = 1.0\n",
    "    PERCEPTUAL_WEIGHT = 0.1  # Perceptual loss weight\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PERCEPTUAL LOSS NETWORK\n",
    "# ==============================================================================\n",
    "\n",
    "class PerceptualLossNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses VGG19 pretrained features to compute perceptual loss.\n",
    "    Extracts features from relu1_2, relu2_2, relu3_2 layers.\n",
    "    Adapted for single-channel (grayscale) input by replicating to 3 channels.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "        # Split VGG into feature extraction stages\n",
    "        # relu1_2: layers 0-3\n",
    "        # relu2_2: layers 4-8\n",
    "        # relu3_2: layers 9-13\n",
    "        self.slice1 = nn.Sequential(*[vgg[i] for i in range(4)])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*[vgg[i] for i in range(4, 9)])   # relu2_2\n",
    "        self.slice3 = nn.Sequential(*[vgg[i] for i in range(9, 14)])  # relu3_2\n",
    "        \n",
    "        # Freeze all VGG parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Feature weights (deeper = more weight)\n",
    "        self.weights = [1.0, 1.0, 1.0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: grayscale image tensor (B, 1, H, W)\n",
    "        Returns:\n",
    "            list of feature maps from each VGG stage\n",
    "        \"\"\"\n",
    "        # Convert grayscale to 3-channel by repeating\n",
    "        x_3ch = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        h1 = self.slice1(x_3ch)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        \n",
    "        return [h1, h2, h3]\n",
    "\n",
    "\n",
    "class CombinedSRLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined L1 + Perceptual loss for super-resolution.\n",
    "    L1 preserves pixel accuracy, perceptual loss preserves structure/texture.\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.perceptual_net = PerceptualLossNetwork().to(device)\n",
    "        self.perceptual_net.eval()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "    \n",
    "    def forward(self, sr_output, hr_target):\n",
    "        # L1 pixel loss\n",
    "        l1 = self.l1_loss(sr_output, hr_target)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        sr_features = self.perceptual_net(sr_output)\n",
    "        hr_features = self.perceptual_net(hr_target)\n",
    "        \n",
    "        perceptual = 0.0\n",
    "        for i, (sf, hf) in enumerate(zip(sr_features, hr_features)):\n",
    "            perceptual += self.perceptual_net.weights[i] * F.l1_loss(sf, hf)\n",
    "        perceptual /= len(sr_features)\n",
    "        \n",
    "        total = self.l1_weight * l1 + self.perceptual_weight * perceptual\n",
    "        return total, l1, perceptual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BUILDING BLOCKS\n",
    "# ==============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v10_perceptual_loss (same architecture as baseline)\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v10_perceptual_loss: Standard LapSRN (same as baseline, loss is different)\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v10_perceptual_loss: Standard DRRN (same as baseline, loss is different)\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v10_perceptual_loss: Standard ResNet50 classifier (same as baseline)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()}\")\n",
    "    print(f\"  - Loss: L1 (weight={config.L1_WEIGHT}) + Perceptual (weight={config.PERCEPTUAL_WEIGHT})\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN().to(config.DEVICE)\n",
    "    drrn = DRRN().to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    # Initialize perceptual loss (shared for both SR models)\n",
    "    combined_loss = CombinedSRLoss(\n",
    "        l1_weight=config.L1_WEIGHT,\n",
    "        perceptual_weight=config.PERCEPTUAL_WEIGHT,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPerceptual Loss info:\")\n",
    "    print(f\"  - Feature extractor: VGG19 (pretrained, frozen)\")\n",
    "    print(f\"  - Feature layers: relu1_2, relu2_2, relu3_2\")\n",
    "    print(f\"  - Grayscale handling: replicate to 3 channels before VGG\")\n",
    "    print(f\"  - Total loss = {config.L1_WEIGHT} * L1 + {config.PERCEPTUAL_WEIGHT} * Perceptual\")\n",
    "    print(f\"  - Expected: Better structural/texture preservation in SR outputs\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Perceptual Loss\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss, l1, perceptual = combined_loss(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}', 'l1': f'{l1.item():.6f}', 'perc': f'{perceptual.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Perceptual Loss\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss, l1, perceptual = combined_loss(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}', 'l1': f'{l1.item():.6f}', 'perc': f'{perceptual.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'l1_weight': config.L1_WEIGHT,\n",
    "        'perceptual_weight': config.PERCEPTUAL_WEIGHT,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Same baseline architecture. SR training uses combined L1 + VGG19 perceptual loss for better structural preservation.'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey difference from v1_baseline:\")\n",
    "    print(\"  - SR loss changed from L1-only to L1 + Perceptual (VGG19)\")\n",
    "    print(\"  - VGG19 features: relu1_2, relu2_2, relu3_2\")\n",
    "    print(\"  - Grayscale input replicated to 3ch before passing through VGG19\")\n",
    "    print(\"  - Expected: Better texture/structure preservation, higher SSIM\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63408c36-6934-43fa-89f2-ce19c4fa085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script - v10_perceptual_loss (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "Note: v10 uses the same baseline architecture as v1. Perceptual loss only affects training.\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_v10_perceptual_loss import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v10_perceptual_loss',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v10'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - v10_perceptual_loss (baseline architecture)\n",
    "# ==============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version='v10_perceptual_loss', data_dir='./preprocessed_data', \n",
    "                   model_dir='./trained_models_v10',\n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels, {config.get('lapsrn_blocks', 5)} blocks (baseline)\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels, {config.get('drrn_blocks', 25)} blocks (baseline)\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\")\n",
    "    print(f\"  Perceptual Loss (training only): L1={config.get('l1_weight', 1.0)}, Perceptual={config.get('perceptual_weight', 0.1)}\\n\")\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    lapsrn = LapSRN().to(device)\n",
    "    drrn = DRRN().to(device)\n",
    "    classifier = MedicalImageClassifier().to(device)\n",
    "    \n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list, lapsrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                lapsrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                lapsrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list, drrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                drrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                drrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds, all_labels, all_urgency_preds, all_urgency_true = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'], output_dict=True)\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': { 'psnr': float(lapsrn_psnr_mean), 'ssim': float(lapsrn_ssim_mean) },\n",
    "        'drrn': { 'psnr': float(drrn_psnr_mean), 'ssim': float(drrn_ssim_mean) },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba26e60-12a3-451b-b318-9d6bfcfa44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(version='v10_perceptual_loss', data_dir='./preprocessed_data', model_dir='./trained_models_v10')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
