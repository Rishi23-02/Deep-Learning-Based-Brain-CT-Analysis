{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc54c2c9-37d7-440f-862b-99c62e0cab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: SimpleITK in /opt/venv/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f53f73-584b-48e6-9f59-e7b702ee5aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V11_MULTI_SCALE_FUSION\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET50\n",
      "  - Multi-Scale Fusion: Enabled (dilation 1, 2, 4)\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Multi-Scale Fusion info:\n",
      "  - Type: Dilated convolution multi-scale branches\n",
      "  - Branch 1 (local):  3x3 conv, dilation=1 → receptive field 3x3\n",
      "  - Branch 2 (mid):    3x3 conv, dilation=2 → receptive field 5x5\n",
      "  - Branch 3 (global): 3x3 conv, dilation=4 → receptive field 9x9\n",
      "  - Fusion: concatenate 3 branches → 1x1 conv → add residual\n",
      "  - LapSRN: applied after upsampling at each pyramid level\n",
      "  - DRRN: applied at each multi-scale collection point (8, 16, 24)\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Multi-Scale Fusion\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1325/1328 [00:41<00:00, 35.61it/s, loss=0.013038]MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e500000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e500000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 9437184, provided ptr: 0x77bd57200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 9437184, provided ptr: 0x77bd57200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 9437184, provided ptr: 0x77bd5e200000 size: 6438912\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 2359296, provided ptr: 0x77bd9bf00000 size: 1720320\n",
      "Epoch 1/50: 100% 1328/1328 [00:42<00:00, 31.56it/s, loss=0.016676]\n",
      "Epoch 2/50: 100% 1328/1328 [00:39<00:00, 33.77it/s, loss=0.020963]\n",
      "Epoch 3/50: 100% 1328/1328 [00:38<00:00, 34.45it/s, loss=0.012300]\n",
      "Epoch 4/50: 100% 1328/1328 [00:38<00:00, 34.06it/s, loss=0.012786]\n",
      "Epoch 5/50: 100% 1328/1328 [00:39<00:00, 33.90it/s, loss=0.009260]\n",
      "Epoch 6/50: 100% 1328/1328 [00:39<00:00, 33.83it/s, loss=0.005688]\n",
      "Epoch 7/50: 100% 1328/1328 [00:39<00:00, 33.63it/s, loss=0.006063]\n",
      "Epoch 8/50: 100% 1328/1328 [00:39<00:00, 34.04it/s, loss=0.027469]\n",
      "Epoch 9/50: 100% 1328/1328 [00:38<00:00, 34.29it/s, loss=0.000555]\n",
      "Epoch 10/50: 100% 1328/1328 [00:38<00:00, 34.25it/s, loss=0.000588]\n",
      "Epoch 11/50: 100% 1328/1328 [00:38<00:00, 34.31it/s, loss=0.012888]\n",
      "Epoch 12/50: 100% 1328/1328 [00:37<00:00, 35.17it/s, loss=0.014458]\n",
      "Epoch 13/50: 100% 1328/1328 [00:38<00:00, 34.50it/s, loss=0.009496]\n",
      "Epoch 14/50: 100% 1328/1328 [00:39<00:00, 34.01it/s, loss=0.010131]\n",
      "Epoch 15/50: 100% 1328/1328 [00:39<00:00, 33.89it/s, loss=0.011440]\n",
      "Epoch 16/50: 100% 1328/1328 [00:39<00:00, 33.86it/s, loss=0.017621]\n",
      "Epoch 17/50: 100% 1328/1328 [00:39<00:00, 33.70it/s, loss=0.008247]\n",
      "Epoch 18/50: 100% 1328/1328 [00:38<00:00, 34.07it/s, loss=0.009390]\n",
      "Epoch 19/50: 100% 1328/1328 [00:39<00:00, 33.90it/s, loss=0.013373]\n",
      "Epoch 20/50: 100% 1328/1328 [00:38<00:00, 34.15it/s, loss=0.013541]\n",
      "Epoch 21/50: 100% 1328/1328 [00:39<00:00, 33.88it/s, loss=0.009933]\n",
      "Epoch 22/50: 100% 1328/1328 [00:39<00:00, 33.45it/s, loss=0.010563]\n",
      "Epoch 23/50: 100% 1328/1328 [00:39<00:00, 33.57it/s, loss=0.017349]\n",
      "Epoch 24/50: 100% 1328/1328 [00:39<00:00, 33.55it/s, loss=0.027277]\n",
      "Epoch 25/50: 100% 1328/1328 [00:39<00:00, 33.68it/s, loss=0.000072]\n",
      "Epoch 26/50: 100% 1328/1328 [00:38<00:00, 34.29it/s, loss=0.022426]\n",
      "Epoch 27/50: 100% 1328/1328 [00:39<00:00, 33.90it/s, loss=0.012025]\n",
      "Epoch 28/50: 100% 1328/1328 [00:39<00:00, 33.93it/s, loss=0.007969]\n",
      "Epoch 29/50: 100% 1328/1328 [00:38<00:00, 34.16it/s, loss=0.004637]\n",
      "Epoch 30/50: 100% 1328/1328 [00:39<00:00, 33.39it/s, loss=0.009727]\n",
      "Epoch 31/50: 100% 1328/1328 [00:39<00:00, 33.91it/s, loss=0.009177]\n",
      "Epoch 32/50: 100% 1328/1328 [00:39<00:00, 33.97it/s, loss=0.014522]\n",
      "Epoch 33/50: 100% 1328/1328 [00:39<00:00, 34.00it/s, loss=0.007116]\n",
      "Epoch 34/50: 100% 1328/1328 [00:39<00:00, 33.42it/s, loss=0.022163]\n",
      "Epoch 35/50: 100% 1328/1328 [00:39<00:00, 33.48it/s, loss=0.000044]\n",
      "Epoch 36/50: 100% 1328/1328 [00:39<00:00, 33.64it/s, loss=0.013640]\n",
      "Epoch 37/50: 100% 1328/1328 [00:37<00:00, 34.97it/s, loss=0.000053]\n",
      "Epoch 38/50: 100% 1328/1328 [00:38<00:00, 34.09it/s, loss=0.018823]\n",
      "Epoch 39/50: 100% 1328/1328 [00:39<00:00, 33.77it/s, loss=0.008184]\n",
      "Epoch 40/50: 100% 1328/1328 [00:39<00:00, 33.75it/s, loss=0.012632]\n",
      "Epoch 41/50: 100% 1328/1328 [00:39<00:00, 34.05it/s, loss=0.005772]\n",
      "Epoch 42/50: 100% 1328/1328 [00:39<00:00, 33.97it/s, loss=0.003039]\n",
      "Epoch 43/50: 100% 1328/1328 [00:39<00:00, 33.98it/s, loss=0.008655]\n",
      "Epoch 44/50: 100% 1328/1328 [00:38<00:00, 34.31it/s, loss=0.019081]\n",
      "Epoch 45/50: 100% 1328/1328 [00:38<00:00, 34.47it/s, loss=0.011596]\n",
      "Epoch 46/50: 100% 1328/1328 [00:40<00:00, 33.10it/s, loss=0.004628]\n",
      "Epoch 47/50: 100% 1328/1328 [00:41<00:00, 32.07it/s, loss=0.016317]\n",
      "Epoch 48/50: 100% 1328/1328 [00:40<00:00, 32.43it/s, loss=0.019498]\n",
      "Epoch 49/50: 100% 1328/1328 [00:41<00:00, 32.25it/s, loss=0.025370]\n",
      "Epoch 50/50: 100% 1328/1328 [00:40<00:00, 32.69it/s, loss=0.000034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.009542)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Multi-Scale Fusion\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1326/1328 [01:32<00:00, 14.35it/s, loss=0.002945]MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 4718592, provided ptr: 0x77bcf2a00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 4718592, provided ptr: 0x77bcf2a00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmFwdRest>, workspace required: 4718592, provided ptr: 0x77bcf2a00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmFwdRest>, workspace required: 4718592, provided ptr: 0x77bcf2a00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmBwdRest>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmBwdRest>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [GetSolutionsFallback AI] Solver <GemmWrwUniversal>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "MIOpen(HIP): Warning [IsEnoughWorkspace] [EvaluateInvokers] Solver <GemmWrwUniversal>, workspace required: 4718592, provided ptr: 0x77bcebc00000 size: 3735552\n",
      "Epoch 1/50: 100% 1328/1328 [01:33<00:00, 14.23it/s, loss=0.001701]\n",
      "Epoch 2/50: 100% 1328/1328 [01:31<00:00, 14.45it/s, loss=0.002877]\n",
      "Epoch 3/50: 100% 1328/1328 [01:32<00:00, 14.41it/s, loss=0.002158]\n",
      "Epoch 4/50: 100% 1328/1328 [01:32<00:00, 14.42it/s, loss=0.004586]\n",
      "Epoch 5/50: 100% 1328/1328 [01:31<00:00, 14.47it/s, loss=0.000446]\n",
      "Epoch 6/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.002746]\n",
      "Epoch 7/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.001312]\n",
      "Epoch 8/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.003010]\n",
      "Epoch 9/50: 100% 1328/1328 [01:31<00:00, 14.45it/s, loss=0.001282]\n",
      "Epoch 10/50: 100% 1328/1328 [01:31<00:00, 14.45it/s, loss=0.000059]\n",
      "Epoch 11/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.002984]\n",
      "Epoch 12/50: 100% 1328/1328 [01:32<00:00, 14.38it/s, loss=0.001435]\n",
      "Epoch 13/50: 100% 1328/1328 [01:32<00:00, 14.36it/s, loss=0.002938]\n",
      "Epoch 14/50: 100% 1328/1328 [01:32<00:00, 14.38it/s, loss=0.004352]\n",
      "Epoch 15/50: 100% 1328/1328 [01:32<00:00, 14.41it/s, loss=0.003042]\n",
      "Epoch 16/50: 100% 1328/1328 [01:32<00:00, 14.39it/s, loss=0.000737]\n",
      "Epoch 17/50: 100% 1328/1328 [01:32<00:00, 14.43it/s, loss=0.000794]\n",
      "Epoch 18/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.003411]\n",
      "Epoch 19/50: 100% 1328/1328 [01:31<00:00, 14.45it/s, loss=0.001928]\n",
      "Epoch 20/50: 100% 1328/1328 [01:32<00:00, 14.40it/s, loss=0.006133]\n",
      "Epoch 21/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.000996]\n",
      "Epoch 22/50: 100% 1328/1328 [01:31<00:00, 14.46it/s, loss=0.001935]\n",
      "Epoch 23/50: 100% 1328/1328 [01:31<00:00, 14.47it/s, loss=0.004742]\n",
      "Epoch 24/50: 100% 1328/1328 [01:32<00:00, 14.43it/s, loss=0.004984]\n",
      "Epoch 25/50: 100% 1328/1328 [01:32<00:00, 14.41it/s, loss=0.001598]\n",
      "Epoch 26/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.000061]\n",
      "Epoch 27/50: 100% 1328/1328 [01:32<00:00, 14.39it/s, loss=0.001309]\n",
      "Epoch 28/50: 100% 1328/1328 [01:32<00:00, 14.42it/s, loss=0.003573]\n",
      "Epoch 29/50: 100% 1328/1328 [01:32<00:00, 14.40it/s, loss=0.003241]\n",
      "Epoch 30/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.001546]\n",
      "Epoch 31/50: 100% 1328/1328 [01:31<00:00, 14.46it/s, loss=0.002088]\n",
      "Epoch 32/50: 100% 1328/1328 [01:31<00:00, 14.46it/s, loss=0.005088]\n",
      "Epoch 33/50: 100% 1328/1328 [01:32<00:00, 14.43it/s, loss=0.002897]\n",
      "Epoch 34/50: 100% 1328/1328 [01:32<00:00, 14.38it/s, loss=0.003565]\n",
      "Epoch 35/50: 100% 1328/1328 [01:31<00:00, 14.48it/s, loss=0.000539]\n",
      "Epoch 36/50: 100% 1328/1328 [01:32<00:00, 14.41it/s, loss=0.003116]\n",
      "Epoch 37/50: 100% 1328/1328 [01:32<00:00, 14.38it/s, loss=0.000685]\n",
      "Epoch 38/50: 100% 1328/1328 [01:32<00:00, 14.42it/s, loss=0.001896]\n",
      "Epoch 39/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.002448]\n",
      "Epoch 40/50: 100% 1328/1328 [01:32<00:00, 14.43it/s, loss=0.002464]\n",
      "Epoch 41/50: 100% 1328/1328 [01:31<00:00, 14.44it/s, loss=0.003461]\n",
      "Epoch 42/50: 100% 1328/1328 [01:32<00:00, 14.41it/s, loss=0.001837]\n",
      "Epoch 43/50: 100% 1328/1328 [01:24<00:00, 15.68it/s, loss=0.005921]\n",
      "Epoch 44/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.004161]\n",
      "Epoch 45/50: 100% 1328/1328 [01:24<00:00, 15.80it/s, loss=0.003089]\n",
      "Epoch 46/50: 100% 1328/1328 [01:26<00:00, 15.44it/s, loss=0.002010]\n",
      "Epoch 47/50: 100% 1328/1328 [01:27<00:00, 15.10it/s, loss=0.001287]\n",
      "Epoch 48/50: 100% 1328/1328 [01:27<00:00, 15.09it/s, loss=0.005423]\n",
      "Epoch 49/50: 100% 1328/1328 [01:27<00:00, 15.12it/s, loss=0.003296]\n",
      "Epoch 50/50: 100% 1328/1328 [01:28<00:00, 15.08it/s, loss=0.000981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.002318)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0% 0/332 [00:00<?, ?it/s]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30:  99% 330/332 [00:26<00:00, 17.98it/s, acc=77.76%]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 332/332 [00:26<00:00, 12.47it/s, acc=77.77%]\n",
      "Epoch 2/30: 100% 332/332 [00:15<00:00, 20.93it/s, acc=86.81%]\n",
      "Epoch 3/30: 100% 332/332 [00:11<00:00, 28.52it/s, acc=90.41%]\n",
      "Epoch 4/30: 100% 332/332 [00:11<00:00, 28.34it/s, acc=93.50%]\n",
      "Epoch 5/30: 100% 332/332 [00:11<00:00, 28.20it/s, acc=94.29%]\n",
      "Epoch 6/30: 100% 332/332 [00:11<00:00, 28.28it/s, acc=94.88%]\n",
      "Epoch 7/30: 100% 332/332 [00:11<00:00, 27.87it/s, acc=95.67%]\n",
      "Epoch 8/30: 100% 332/332 [00:11<00:00, 28.67it/s, acc=96.10%]\n",
      "Epoch 9/30: 100% 332/332 [00:11<00:00, 28.95it/s, acc=96.04%]\n",
      "Epoch 10/30: 100% 332/332 [00:11<00:00, 28.42it/s, acc=96.36%]\n",
      "Epoch 11/30: 100% 332/332 [00:11<00:00, 27.97it/s, acc=96.50%]\n",
      "Epoch 12/30: 100% 332/332 [00:11<00:00, 28.77it/s, acc=97.38%]\n",
      "Epoch 13/30: 100% 332/332 [00:11<00:00, 28.67it/s, acc=96.53%]\n",
      "Epoch 14/30: 100% 332/332 [00:11<00:00, 28.39it/s, acc=97.12%]\n",
      "Epoch 15/30: 100% 332/332 [00:11<00:00, 28.63it/s, acc=97.49%]\n",
      "Epoch 16/30: 100% 332/332 [00:11<00:00, 28.69it/s, acc=97.17%]\n",
      "Epoch 17/30: 100% 332/332 [00:11<00:00, 28.29it/s, acc=96.95%]\n",
      "Epoch 18/30: 100% 332/332 [00:11<00:00, 28.24it/s, acc=97.29%]\n",
      "Epoch 19/30: 100% 332/332 [00:11<00:00, 28.95it/s, acc=96.93%]\n",
      "Epoch 20/30: 100% 332/332 [00:11<00:00, 28.59it/s, acc=96.89%]\n",
      "Epoch 21/30: 100% 332/332 [00:11<00:00, 28.90it/s, acc=97.29%]\n",
      "Epoch 22/30: 100% 332/332 [00:11<00:00, 28.62it/s, acc=97.85%]\n",
      "Epoch 23/30: 100% 332/332 [00:11<00:00, 29.00it/s, acc=97.66%]\n",
      "Epoch 24/30: 100% 332/332 [00:11<00:00, 28.71it/s, acc=97.17%]\n",
      "Epoch 25/30: 100% 332/332 [00:11<00:00, 28.71it/s, acc=97.65%]\n",
      "Epoch 26/30: 100% 332/332 [00:11<00:00, 28.83it/s, acc=97.27%]\n",
      "Epoch 27/30: 100% 332/332 [00:11<00:00, 28.58it/s, acc=97.59%]\n",
      "Epoch 28/30: 100% 332/332 [00:11<00:00, 28.61it/s, acc=97.95%]\n",
      "Epoch 29/30: 100% 332/332 [00:11<00:00, 28.20it/s, acc=98.21%]\n",
      "Epoch 30/30: 100% 332/332 [00:12<00:00, 27.62it/s, acc=97.27%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classifier training complete (best accuracy: 98.21%)\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Models saved to: ./trained_models_v11/v11_multi_scale_fusion\n",
      "\n",
      "Pipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\n",
      "\n",
      "Key difference from v1_baseline:\n",
      "  - Added MultiScaleFusionBlock using 3 parallel dilated conv branches\n",
      "  - Dilation rates 1, 2, 4 capture local, mid-range, and global context\n",
      "  - LapSRN: fusion applied after upsampling at each pyramid level\n",
      "  - DRRN: fusion applied at collection points (8, 16, 24)\n",
      "  - Expected: Better context-aware reconstruction, improved edge detail\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v11_multi_scale_fusion (Multi-Scale Feature Fusion)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | Multi-Scale Fusion\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v11_multi_scale_fusion'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v11'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v11_multi_scale_fusion specific - MULTI-SCALE FEATURE FUSION\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet50'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MULTI-SCALE FUSION MODULES\n",
    "# ==============================================================================\n",
    "\n",
    "class MultiScaleFusionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts features at 3 different receptive field sizes using dilated convolutions,\n",
    "    then fuses them with a learned weighted sum via a 1x1 conv.\n",
    "    Captures local detail (dilation=1), mid-range (dilation=2), and global context (dilation=4).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Three parallel branches with different dilation rates\n",
    "        self.branch_local = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1, dilation=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.branch_mid = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=2, dilation=2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.branch_global = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=4, dilation=4),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv fuses the 3 concatenated branches back to original channels\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(channels * 3, channels, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        local_feat = self.branch_local(x)\n",
    "        mid_feat = self.branch_mid(x)\n",
    "        global_feat = self.branch_global(x)\n",
    "        \n",
    "        fused = torch.cat([local_feat, mid_feat, global_feat], dim=1)\n",
    "        out = self.fuse(fused)\n",
    "        \n",
    "        # Global residual\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v11_multi_scale_fusion\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v11_multi_scale_fusion: LapSRN with MultiScaleFusionBlock after each pyramid level\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        self.multi_scale_blocks = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            # Multi-scale fusion applied after upsampling at each level\n",
    "            self.multi_scale_blocks.append(MultiScaleFusionBlock(ch))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            # Apply multi-scale fusion after upsampling\n",
    "            current_features = self.multi_scale_blocks[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v11_multi_scale_fusion: DRRN with MultiScaleFusionBlock at collection points\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        # Multi-scale fusion at each collection point\n",
    "        self.multi_scale_blocks = nn.ModuleList([\n",
    "            MultiScaleFusionBlock(ch),\n",
    "            MultiScaleFusionBlock(ch),\n",
    "            MultiScaleFusionBlock(ch)\n",
    "        ])\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        ms_idx = 0\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                # Apply multi-scale fusion before collecting\n",
    "                current = self.multi_scale_blocks[ms_idx](current)\n",
    "                ms_idx += 1\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v11_multi_scale_fusion: Standard ResNet50 classifier (same as baseline)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()}\")\n",
    "    print(f\"  - Multi-Scale Fusion: Enabled (dilation 1, 2, 4)\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN().to(config.DEVICE)\n",
    "    drrn = DRRN().to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    print(f\"\\nMulti-Scale Fusion info:\")\n",
    "    print(f\"  - Type: Dilated convolution multi-scale branches\")\n",
    "    print(f\"  - Branch 1 (local):  3x3 conv, dilation=1 → receptive field 3x3\")\n",
    "    print(f\"  - Branch 2 (mid):    3x3 conv, dilation=2 → receptive field 5x5\")\n",
    "    print(f\"  - Branch 3 (global): 3x3 conv, dilation=4 → receptive field 9x9\")\n",
    "    print(f\"  - Fusion: concatenate 3 branches → 1x1 conv → add residual\")\n",
    "    print(f\"  - LapSRN: applied after upsampling at each pyramid level\")\n",
    "    print(f\"  - DRRN: applied at each multi-scale collection point (8, 16, 24)\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Multi-Scale Fusion\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.L1Loss()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Multi-Scale Fusion\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Added MultiScaleFusionBlock using dilated convolutions (dilation 1, 2, 4) for capturing local, mid-range, and global context simultaneously.'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey difference from v1_baseline:\")\n",
    "    print(\"  - Added MultiScaleFusionBlock using 3 parallel dilated conv branches\")\n",
    "    print(\"  - Dilation rates 1, 2, 4 capture local, mid-range, and global context\")\n",
    "    print(\"  - LapSRN: fusion applied after upsampling at each pyramid level\")\n",
    "    print(\"  - DRRN: fusion applied at collection points (8, 16, 24)\")\n",
    "    print(\"  - Expected: Better context-aware reconstruction, improved edge detail\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291de32b-3c4f-4aa6-8f5b-e9baf8fba5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script - v11_multi_scale_fusion (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_v11_multi_scale_fusion import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v11_multi_scale_fusion',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v11'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - v11_multi_scale_fusion\n",
    "# ==============================================================================\n",
    "\n",
    "class MultiScaleFusionBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.branch_local = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1, dilation=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.branch_mid = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=2, dilation=2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.branch_global = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=4, dilation=4),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(channels * 3, channels, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        local_feat = self.branch_local(x)\n",
    "        mid_feat = self.branch_mid(x)\n",
    "        global_feat = self.branch_global(x)\n",
    "        \n",
    "        fused = torch.cat([local_feat, mid_feat, global_feat], dim=1)\n",
    "        out = self.fuse(fused)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        self.multi_scale_blocks = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.multi_scale_blocks.append(MultiScaleFusionBlock(ch))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            current_features = self.multi_scale_blocks[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.multi_scale_blocks = nn.ModuleList([\n",
    "            MultiScaleFusionBlock(ch),\n",
    "            MultiScaleFusionBlock(ch),\n",
    "            MultiScaleFusionBlock(ch)\n",
    "        ])\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        ms_idx = 0\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                current = self.multi_scale_blocks[ms_idx](current)\n",
    "                ms_idx += 1\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version='v11_multi_scale_fusion', data_dir='./preprocessed_data', \n",
    "                   model_dir='./trained_models_v11',\n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels + MultiScaleFusion (dilation 1,2,4)\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels + MultiScaleFusion (dilation 1,2,4)\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\\n\")\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    lapsrn = LapSRN().to(device)\n",
    "    drrn = DRRN().to(device)\n",
    "    classifier = MedicalImageClassifier().to(device)\n",
    "    \n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list, lapsrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                lapsrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                lapsrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list, drrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                drrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                drrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds, all_labels, all_urgency_preds, all_urgency_true = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'], output_dict=True)\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': { 'psnr': float(lapsrn_psnr_mean), 'ssim': float(lapsrn_ssim_mean) },\n",
    "        'drrn': { 'psnr': float(drrn_psnr_mean), 'ssim': float(drrn_ssim_mean) },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b507c7ed-2a4c-4255-9125-46620027c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING MODEL: v11_multi_scale_fusion\n",
      "================================================================================\n",
      "\n",
      "Loaded configuration for v11_multi_scale_fusion\n",
      "  LapSRN: 64 channels + MultiScaleFusion (dilation 1,2,4)\n",
      "  DRRN: 128 channels + MultiScaleFusion (dilation 1,2,4)\n",
      "  Backbone: resnet50\n",
      "\n",
      "Creating datasets...\n",
      "  SR dataset: 6636 samples\n",
      "  DRRN dataset: 6636 samples\n",
      "  Classification dataset: 6636 samples\n",
      "\n",
      "Loading models...\n",
      "✓ Models loaded successfully\n",
      "\n",
      "================================================================================\n",
      "[1/3] Evaluating LapSRN (16x16 → 64x64)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LapSRN Evaluation: 100% 415/415 [00:12<00:00, 33.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ LapSRN Results:\n",
      "  PSNR: 33.6089 dB\n",
      "  SSIM: 0.8248\n",
      "\n",
      "================================================================================\n",
      "[2/3] Evaluating DRRN (64x64 → 128x128)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DRRN Evaluation: 100% 415/415 [00:12<00:00, 34.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DRRN Results:\n",
      "  PSNR: 46.0370 dB\n",
      "  SSIM: 0.9838\n",
      "\n",
      "================================================================================\n",
      "[3/3] Evaluating Classifier\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Evaluation: 100% 415/415 [00:14<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification Results:\n",
      "  Accuracy: 96.93%\n",
      "\n",
      "  Confusion Matrix:\n",
      "  [[4409   18    0]\n",
      " [  74 1040    2]\n",
      " [  95   15  983]]\n",
      "\n",
      "  Per-Class Metrics:\n",
      "    Normal:\n",
      "      Precision: 0.9631\n",
      "      Recall: 0.9959\n",
      "      F1-Score: 0.9792\n",
      "    Ischemia:\n",
      "      Precision: 0.9692\n",
      "      Recall: 0.9319\n",
      "      F1-Score: 0.9502\n",
      "    Bleeding:\n",
      "      Precision: 0.9980\n",
      "      Recall: 0.8994\n",
      "      F1-Score: 0.9461\n",
      "\n",
      "  Urgency Prediction:\n",
      "    MSE: 0.0124\n",
      "    MAE: 0.0505\n",
      "\n",
      "✓ Results saved to: ./trained_models_v11/v11_multi_scale_fusion/evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE FOR v11_multi_scale_fusion\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(version='v11_multi_scale_fusion', data_dir='./preprocessed_data', model_dir='./trained_models_v11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b1cfc-ebec-483b-b5e5-447cfc59437a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
