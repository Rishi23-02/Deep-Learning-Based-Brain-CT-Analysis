{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e672fe8d-b36a-4570-959f-110eb7aca17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: SimpleITK in /opt/venv/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53634dd6-cf35-4e80-9b72-11b3beffdeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V12_COMBINED\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET50\n",
      "  - CBAM Attention: Enabled (from v7)\n",
      "  - Batch Normalization: Enabled (from v8)\n",
      "  - Perceptual Loss: Enabled (from v10)\n",
      "  - Loss: L1 (weight=1.0) + Perceptual (weight=0.1)\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Combined techniques info:\n",
      "  [v7] CBAM Attention: Channel + Spatial attention after every block\n",
      "  [v8] Batch Normalization: After every convolution layer\n",
      "  [v10] Perceptual Loss: VGG19 features (relu1_2, relu2_2, relu3_2)\n",
      "  Block order: Conv → BatchNorm → LeakyReLU → Conv → BatchNorm → CBAM → Residual\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64) + CBAM + BatchNorm + Perceptual Loss\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [01:22<00:00, 16.13it/s, loss=0.091899, l1=0.053724, perc=0.381748]\n",
      "Epoch 2/50: 100% 1328/1328 [01:20<00:00, 16.53it/s, loss=0.037310, l1=0.021958, perc=0.153520]\n",
      "Epoch 3/50: 100% 1328/1328 [01:20<00:00, 16.56it/s, loss=0.053396, l1=0.030600, perc=0.227961]\n",
      "Epoch 4/50: 100% 1328/1328 [01:20<00:00, 16.40it/s, loss=0.086282, l1=0.050556, perc=0.357260]\n",
      "Epoch 5/50: 100% 1328/1328 [01:19<00:00, 16.64it/s, loss=0.052425, l1=0.031477, perc=0.209485]\n",
      "Epoch 6/50: 100% 1328/1328 [01:19<00:00, 16.69it/s, loss=0.063155, l1=0.031420, perc=0.317350]\n",
      "Epoch 7/50: 100% 1328/1328 [01:20<00:00, 16.49it/s, loss=0.014554, l1=0.007849, perc=0.067052]\n",
      "Epoch 8/50: 100% 1328/1328 [01:19<00:00, 16.75it/s, loss=0.034520, l1=0.018420, perc=0.161001]\n",
      "Epoch 9/50: 100% 1328/1328 [01:18<00:00, 16.86it/s, loss=0.018240, l1=0.010945, perc=0.072946]\n",
      "Epoch 10/50: 100% 1328/1328 [01:19<00:00, 16.69it/s, loss=0.012096, l1=0.007777, perc=0.043186]\n",
      "Epoch 11/50: 100% 1328/1328 [01:20<00:00, 16.56it/s, loss=0.022107, l1=0.012247, perc=0.098604]\n",
      "Epoch 12/50: 100% 1328/1328 [01:19<00:00, 16.69it/s, loss=0.015560, l1=0.009054, perc=0.065060]\n",
      "Epoch 13/50: 100% 1328/1328 [01:19<00:00, 16.64it/s, loss=0.016622, l1=0.009100, perc=0.075226]\n",
      "Epoch 14/50: 100% 1328/1328 [01:19<00:00, 16.72it/s, loss=0.009366, l1=0.005077, perc=0.042891]\n",
      "Epoch 15/50: 100% 1328/1328 [01:19<00:00, 16.64it/s, loss=0.010049, l1=0.006338, perc=0.037111]\n",
      "Epoch 16/50: 100% 1328/1328 [01:31<00:00, 14.45it/s, loss=0.053516, l1=0.031091, perc=0.224259]\n",
      "Epoch 17/50: 100% 1328/1328 [01:50<00:00, 12.05it/s, loss=0.020461, l1=0.011617, perc=0.088448]\n",
      "Epoch 18/50: 100% 1328/1328 [01:51<00:00, 11.93it/s, loss=0.015508, l1=0.009096, perc=0.064114]\n",
      "Epoch 19/50: 100% 1328/1328 [02:00<00:00, 10.99it/s, loss=0.038098, l1=0.023805, perc=0.142930]\n",
      "Epoch 20/50: 100% 1328/1328 [02:04<00:00, 10.66it/s, loss=0.026551, l1=0.014744, perc=0.118069]\n",
      "Epoch 21/50: 100% 1328/1328 [02:18<00:00,  9.58it/s, loss=0.022105, l1=0.012195, perc=0.099099]\n",
      "Epoch 22/50: 100% 1328/1328 [02:18<00:00,  9.60it/s, loss=0.016201, l1=0.009994, perc=0.062076]\n",
      "Epoch 23/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.021136, l1=0.012852, perc=0.082841]\n",
      "Epoch 24/50: 100% 1328/1328 [02:18<00:00,  9.58it/s, loss=0.022530, l1=0.012388, perc=0.101424]\n",
      "Epoch 25/50: 100% 1328/1328 [02:19<00:00,  9.55it/s, loss=0.035742, l1=0.020793, perc=0.149482]\n",
      "Epoch 26/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.016424, l1=0.009274, perc=0.071505]\n",
      "Epoch 27/50: 100% 1328/1328 [02:18<00:00,  9.60it/s, loss=0.033745, l1=0.018718, perc=0.150268]\n",
      "Epoch 28/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.026819, l1=0.014780, perc=0.120396]\n",
      "Epoch 29/50: 100% 1328/1328 [02:18<00:00,  9.60it/s, loss=0.013861, l1=0.009038, perc=0.048228]\n",
      "Epoch 30/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.057176, l1=0.031620, perc=0.255569]\n",
      "Epoch 31/50: 100% 1328/1328 [02:18<00:00,  9.58it/s, loss=0.027750, l1=0.016847, perc=0.109032]\n",
      "Epoch 32/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.024617, l1=0.013670, perc=0.109473]\n",
      "Epoch 33/50: 100% 1328/1328 [02:18<00:00,  9.59it/s, loss=0.015824, l1=0.008429, perc=0.073949]\n",
      "Epoch 34/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.017823, l1=0.011384, perc=0.064388]\n",
      "Epoch 35/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.012892, l1=0.008027, perc=0.048652]\n",
      "Epoch 36/50: 100% 1328/1328 [02:19<00:00,  9.55it/s, loss=0.017930, l1=0.009825, perc=0.081048]\n",
      "Epoch 37/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.008927, l1=0.005511, perc=0.034167]\n",
      "Epoch 38/50: 100% 1328/1328 [02:18<00:00,  9.59it/s, loss=0.025266, l1=0.014703, perc=0.105629]\n",
      "Epoch 39/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.038633, l1=0.021631, perc=0.170021]\n",
      "Epoch 40/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.008698, l1=0.004887, perc=0.038115]\n",
      "Epoch 41/50: 100% 1328/1328 [02:18<00:00,  9.61it/s, loss=0.019143, l1=0.010313, perc=0.088303]\n",
      "Epoch 42/50: 100% 1328/1328 [02:19<00:00,  9.55it/s, loss=0.013402, l1=0.007224, perc=0.061778]\n",
      "Epoch 43/50: 100% 1328/1328 [02:18<00:00,  9.58it/s, loss=0.065874, l1=0.037402, perc=0.284713]\n",
      "Epoch 44/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.038741, l1=0.021689, perc=0.170521]\n",
      "Epoch 45/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.024390, l1=0.013148, perc=0.112419]\n",
      "Epoch 46/50: 100% 1328/1328 [02:18<00:00,  9.56it/s, loss=0.031564, l1=0.020270, perc=0.112944]\n",
      "Epoch 47/50: 100% 1328/1328 [02:18<00:00,  9.57it/s, loss=0.018234, l1=0.009772, perc=0.084622]\n",
      "Epoch 48/50: 100% 1328/1328 [02:19<00:00,  9.54it/s, loss=0.029989, l1=0.016046, perc=0.139434]\n",
      "Epoch 49/50: 100% 1328/1328 [02:15<00:00,  9.81it/s, loss=0.013926, l1=0.007577, perc=0.063485]\n",
      "Epoch 50/50: 100% 1328/1328 [02:09<00:00, 10.27it/s, loss=0.021204, l1=0.012231, perc=0.089727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.019023)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128) + CBAM + BatchNorm + Perceptual Loss\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [05:26<00:00,  4.06it/s, loss=0.022423, l1=0.013033, perc=0.093897]\n",
      "Epoch 2/50: 100% 1328/1328 [04:40<00:00,  4.73it/s, loss=0.014157, l1=0.008345, perc=0.058122]\n",
      "Epoch 3/50: 100% 1328/1328 [04:12<00:00,  5.25it/s, loss=0.020187, l1=0.011433, perc=0.087539]\n",
      "Epoch 4/50: 100% 1328/1328 [03:52<00:00,  5.72it/s, loss=0.012008, l1=0.007094, perc=0.049139]\n",
      "Epoch 5/50: 100% 1328/1328 [03:34<00:00,  6.21it/s, loss=0.013705, l1=0.007639, perc=0.060658]\n",
      "Epoch 6/50: 100% 1328/1328 [03:20<00:00,  6.61it/s, loss=0.011588, l1=0.006051, perc=0.055376]\n",
      "Epoch 7/50: 100% 1328/1328 [02:45<00:00,  8.05it/s, loss=0.012300, l1=0.006998, perc=0.053022]\n",
      "Epoch 8/50: 100% 1328/1328 [02:16<00:00,  9.73it/s, loss=0.006155, l1=0.003527, perc=0.026276]\n",
      "Epoch 9/50: 100% 1328/1328 [01:36<00:00, 13.76it/s, loss=0.004859, l1=0.002596, perc=0.022638]\n",
      "Epoch 10/50: 100% 1328/1328 [01:14<00:00, 17.74it/s, loss=0.035415, l1=0.015391, perc=0.200243]\n",
      "Epoch 11/50: 100% 1328/1328 [01:14<00:00, 17.75it/s, loss=0.015746, l1=0.008740, perc=0.070058]\n",
      "Epoch 12/50: 100% 1328/1328 [01:14<00:00, 17.73it/s, loss=0.004002, l1=0.002067, perc=0.019348]\n",
      "Epoch 13/50: 100% 1328/1328 [01:14<00:00, 17.78it/s, loss=0.022518, l1=0.013046, perc=0.094720]\n",
      "Epoch 14/50: 100% 1328/1328 [01:14<00:00, 17.79it/s, loss=0.005745, l1=0.003205, perc=0.025400]\n",
      "Epoch 15/50: 100% 1328/1328 [01:14<00:00, 17.76it/s, loss=0.009970, l1=0.005451, perc=0.045194]\n",
      "Epoch 16/50: 100% 1328/1328 [01:14<00:00, 17.76it/s, loss=0.012386, l1=0.006825, perc=0.055610]\n",
      "Epoch 17/50: 100% 1328/1328 [01:14<00:00, 17.78it/s, loss=0.015694, l1=0.008477, perc=0.072175]\n",
      "Epoch 18/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.010592, l1=0.005640, perc=0.049514]\n",
      "Epoch 19/50: 100% 1328/1328 [01:14<00:00, 17.78it/s, loss=0.005279, l1=0.002988, perc=0.022912]\n",
      "Epoch 20/50: 100% 1328/1328 [01:14<00:00, 17.79it/s, loss=0.004145, l1=0.002249, perc=0.018955]\n",
      "Epoch 21/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.004589, l1=0.002401, perc=0.021876]\n",
      "Epoch 22/50: 100% 1328/1328 [01:14<00:00, 17.75it/s, loss=0.002713, l1=0.001417, perc=0.012963]\n",
      "Epoch 23/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.008259, l1=0.004495, perc=0.037641]\n",
      "Epoch 24/50: 100% 1328/1328 [01:14<00:00, 17.75it/s, loss=0.003094, l1=0.001750, perc=0.013440]\n",
      "Epoch 25/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.001282, l1=0.000577, perc=0.007054]\n",
      "Epoch 26/50: 100% 1328/1328 [01:14<00:00, 17.75it/s, loss=0.004903, l1=0.002646, perc=0.022575]\n",
      "Epoch 27/50: 100% 1328/1328 [01:14<00:00, 17.75it/s, loss=0.003341, l1=0.002010, perc=0.013315]\n",
      "Epoch 28/50: 100% 1328/1328 [01:14<00:00, 17.78it/s, loss=0.003926, l1=0.002038, perc=0.018877]\n",
      "Epoch 29/50: 100% 1328/1328 [01:14<00:00, 17.77it/s, loss=0.008526, l1=0.004839, perc=0.036867]\n",
      "Epoch 30/50: 100% 1328/1328 [01:14<00:00, 17.78it/s, loss=0.011007, l1=0.004272, perc=0.067351]\n",
      "Epoch 31/50: 100% 1328/1328 [01:11<00:00, 18.58it/s, loss=0.009438, l1=0.005485, perc=0.039534]\n",
      "Epoch 32/50: 100% 1328/1328 [01:09<00:00, 19.18it/s, loss=0.010605, l1=0.004538, perc=0.060677]\n",
      "Epoch 33/50: 100% 1328/1328 [01:09<00:00, 19.14it/s, loss=0.007816, l1=0.004227, perc=0.035888]\n",
      "Epoch 34/50: 100% 1328/1328 [01:09<00:00, 19.13it/s, loss=0.004627, l1=0.002442, perc=0.021854]\n",
      "Epoch 35/50: 100% 1328/1328 [01:09<00:00, 19.13it/s, loss=0.002132, l1=0.001086, perc=0.010464]\n",
      "Epoch 36/50: 100% 1328/1328 [01:07<00:00, 19.63it/s, loss=0.006671, l1=0.003628, perc=0.030438]\n",
      "Epoch 37/50: 100% 1328/1328 [01:00<00:00, 21.78it/s, loss=0.005854, l1=0.003066, perc=0.027882]\n",
      "Epoch 38/50: 100% 1328/1328 [01:00<00:00, 22.07it/s, loss=0.001984, l1=0.001107, perc=0.008764]\n",
      "Epoch 39/50: 100% 1328/1328 [01:01<00:00, 21.77it/s, loss=0.004621, l1=0.002107, perc=0.025140]\n",
      "Epoch 40/50: 100% 1328/1328 [01:00<00:00, 21.94it/s, loss=0.007965, l1=0.004397, perc=0.035681]\n",
      "Epoch 41/50: 100% 1328/1328 [01:00<00:00, 22.07it/s, loss=0.007159, l1=0.004046, perc=0.031136]\n",
      "Epoch 42/50: 100% 1328/1328 [00:59<00:00, 22.17it/s, loss=0.003911, l1=0.002071, perc=0.018395]\n",
      "Epoch 43/50: 100% 1328/1328 [01:00<00:00, 22.10it/s, loss=0.003359, l1=0.001496, perc=0.018631]\n",
      "Epoch 44/50: 100% 1328/1328 [01:00<00:00, 22.03it/s, loss=0.010215, l1=0.005653, perc=0.045619]\n",
      "Epoch 45/50: 100% 1328/1328 [01:00<00:00, 22.10it/s, loss=0.004902, l1=0.002512, perc=0.023896]\n",
      "Epoch 46/50: 100% 1328/1328 [00:59<00:00, 22.16it/s, loss=0.004506, l1=0.002495, perc=0.020113]\n",
      "Epoch 47/50: 100% 1328/1328 [01:00<00:00, 22.07it/s, loss=0.007105, l1=0.003873, perc=0.032318]\n",
      "Epoch 48/50: 100% 1328/1328 [01:00<00:00, 22.02it/s, loss=0.005322, l1=0.001736, perc=0.035863]\n",
      "Epoch 49/50: 100% 1328/1328 [01:00<00:00, 22.05it/s, loss=0.004192, l1=0.002246, perc=0.019461]\n",
      "Epoch 50/50: 100% 1328/1328 [01:00<00:00, 22.00it/s, loss=0.012120, l1=0.006702, perc=0.054177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.004685)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100% 332/332 [00:19<00:00, 17.43it/s, acc=80.16%]\n",
      "Epoch 2/30: 100% 332/332 [00:11<00:00, 28.86it/s, acc=87.89%]\n",
      "Epoch 3/30: 100% 332/332 [00:11<00:00, 28.97it/s, acc=91.77%]\n",
      "Epoch 4/30: 100% 332/332 [00:11<00:00, 29.15it/s, acc=93.44%]\n",
      "Epoch 5/30: 100% 332/332 [00:11<00:00, 29.02it/s, acc=94.65%]\n",
      "Epoch 6/30: 100% 332/332 [00:11<00:00, 28.90it/s, acc=96.18%]\n",
      "Epoch 7/30: 100% 332/332 [00:11<00:00, 28.89it/s, acc=96.16%]\n",
      "Epoch 8/30: 100% 332/332 [00:11<00:00, 28.98it/s, acc=95.78%]\n",
      "Epoch 9/30: 100% 332/332 [00:11<00:00, 28.96it/s, acc=96.67%]\n",
      "Epoch 10/30: 100% 332/332 [00:11<00:00, 29.02it/s, acc=96.95%]\n",
      "Epoch 11/30: 100% 332/332 [00:11<00:00, 28.94it/s, acc=96.68%]\n",
      "Epoch 12/30: 100% 332/332 [00:11<00:00, 28.68it/s, acc=96.87%]\n",
      "Epoch 13/30: 100% 332/332 [00:11<00:00, 28.99it/s, acc=96.78%]\n",
      "Epoch 14/30: 100% 332/332 [00:11<00:00, 29.04it/s, acc=97.25%]\n",
      "Epoch 15/30: 100% 332/332 [00:11<00:00, 28.78it/s, acc=97.32%]\n",
      "Epoch 16/30: 100% 332/332 [00:11<00:00, 28.78it/s, acc=97.32%]\n",
      "Epoch 17/30: 100% 332/332 [00:11<00:00, 28.31it/s, acc=97.12%]\n",
      "Epoch 18/30: 100% 332/332 [00:11<00:00, 29.10it/s, acc=97.40%]\n",
      "Epoch 19/30: 100% 332/332 [00:11<00:00, 29.31it/s, acc=97.48%]\n",
      "Epoch 20/30: 100% 332/332 [00:11<00:00, 29.23it/s, acc=97.70%]\n",
      "Epoch 21/30: 100% 332/332 [00:11<00:00, 28.70it/s, acc=96.93%]\n",
      "Epoch 22/30: 100% 332/332 [00:11<00:00, 28.89it/s, acc=97.14%]\n",
      "Epoch 23/30: 100% 332/332 [00:11<00:00, 29.05it/s, acc=97.32%]\n",
      "Epoch 24/30: 100% 332/332 [00:11<00:00, 29.16it/s, acc=98.08%]\n",
      "Epoch 25/30: 100% 332/332 [00:11<00:00, 28.94it/s, acc=97.32%]\n",
      "Epoch 26/30: 100% 332/332 [00:11<00:00, 29.09it/s, acc=97.74%]\n",
      "Epoch 27/30: 100% 332/332 [00:11<00:00, 29.31it/s, acc=97.38%]\n",
      "Epoch 28/30: 100% 332/332 [00:11<00:00, 29.27it/s, acc=97.59%]\n",
      "Epoch 29/30: 100% 332/332 [00:11<00:00, 29.07it/s, acc=97.55%]\n",
      "Epoch 30/30: 100% 332/332 [00:11<00:00, 29.12it/s, acc=97.17%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classifier training complete (best accuracy: 98.08%)\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Models saved to: ./trained_models_v12/v12_combined\n",
      "\n",
      "Pipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\n",
      "\n",
      "Key differences from v1_baseline (3 techniques combined):\n",
      "  [v7]  CBAM: Channel + Spatial attention after every block\n",
      "  [v8]  BatchNorm: After every convolution for training stability\n",
      "  [v10] Perceptual Loss: VGG19 feature matching for texture preservation\n",
      "  Block order: Conv → BN → Act → Conv → BN → CBAM → Residual\n",
      "  Expected: Best overall SR quality combining all individual improvements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v12_combined (CBAM + BatchNorm + Perceptual Loss)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | Combined Best Techniques\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v12_combined'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v12'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v12_combined - COMBINES: CBAM Attention + BatchNorm + Perceptual Loss\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet50'\n",
    "    USE_ATTENTION = True       # From v7\n",
    "    USE_BATCH_NORM = True      # From v8\n",
    "    \n",
    "    # Perceptual loss weights (from v10)\n",
    "    L1_WEIGHT = 1.0\n",
    "    PERCEPTUAL_WEIGHT = 0.1\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CBAM ATTENTION MODULES (from v7)\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(out))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        return out * self.spatial_attention(out)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PERCEPTUAL LOSS NETWORK (from v10)\n",
    "# ==============================================================================\n",
    "\n",
    "class PerceptualLossNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "        self.slice1 = nn.Sequential(*[vgg[i] for i in range(4)])    # relu1_2\n",
    "        self.slice2 = nn.Sequential(*[vgg[i] for i in range(4, 9)])  # relu2_2\n",
    "        self.slice3 = nn.Sequential(*[vgg[i] for i in range(9, 14)]) # relu3_2\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.weights = [1.0, 1.0, 1.0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_3ch = x.repeat(1, 3, 1, 1)\n",
    "        h1 = self.slice1(x_3ch)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        return [h1, h2, h3]\n",
    "\n",
    "\n",
    "class CombinedSRLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.perceptual_net = PerceptualLossNetwork().to(device)\n",
    "        self.perceptual_net.eval()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "    \n",
    "    def forward(self, sr_output, hr_target):\n",
    "        l1 = self.l1_loss(sr_output, hr_target)\n",
    "        \n",
    "        sr_features = self.perceptual_net(sr_output)\n",
    "        hr_features = self.perceptual_net(hr_target)\n",
    "        \n",
    "        perceptual = 0.0\n",
    "        for i, (sf, hf) in enumerate(zip(sr_features, hr_features)):\n",
    "            perceptual += self.perceptual_net.weights[i] * F.l1_loss(sf, hf)\n",
    "        perceptual /= len(sr_features)\n",
    "        \n",
    "        total = self.l1_weight * l1 + self.perceptual_weight * perceptual\n",
    "        return total, l1, perceptual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BUILDING BLOCKS - CBAM + BatchNorm combined (v7 + v8)\n",
    "# ==============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with both BatchNorm and CBAM attention\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.attention(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"Recursive block with both BatchNorm and CBAM attention\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.activation(self.bn2(self.conv2(out)))\n",
    "        out = self.attention(out)\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v12_combined\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v12_combined: LapSRN with CBAM + BatchNorm\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(ch))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v12_combined: DRRN with CBAM + BatchNorm\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v12_combined: Standard ResNet50 classifier (same as baseline)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()}\")\n",
    "    print(f\"  - CBAM Attention: Enabled (from v7)\")\n",
    "    print(f\"  - Batch Normalization: Enabled (from v8)\")\n",
    "    print(f\"  - Perceptual Loss: Enabled (from v10)\")\n",
    "    print(f\"  - Loss: L1 (weight={config.L1_WEIGHT}) + Perceptual (weight={config.PERCEPTUAL_WEIGHT})\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN().to(config.DEVICE)\n",
    "    drrn = DRRN().to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    # Initialize perceptual loss\n",
    "    combined_loss = CombinedSRLoss(\n",
    "        l1_weight=config.L1_WEIGHT,\n",
    "        perceptual_weight=config.PERCEPTUAL_WEIGHT,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined techniques info:\")\n",
    "    print(f\"  [v7] CBAM Attention: Channel + Spatial attention after every block\")\n",
    "    print(f\"  [v8] Batch Normalization: After every convolution layer\")\n",
    "    print(f\"  [v10] Perceptual Loss: VGG19 features (relu1_2, relu2_2, relu3_2)\")\n",
    "    print(f\"  Block order: Conv → BatchNorm → LeakyReLU → Conv → BatchNorm → CBAM → Residual\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64) + CBAM + BatchNorm + Perceptual Loss\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss, l1, perceptual = combined_loss(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}', 'l1': f'{l1.item():.6f}', 'perc': f'{perceptual.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128) + CBAM + BatchNorm + Perceptual Loss\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss, l1, perceptual = combined_loss(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}', 'l1': f'{l1.item():.6f}', 'perc': f'{perceptual.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'use_attention': config.USE_ATTENTION,\n",
    "        'use_batch_norm': config.USE_BATCH_NORM,\n",
    "        'l1_weight': config.L1_WEIGHT,\n",
    "        'perceptual_weight': config.PERCEPTUAL_WEIGHT,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Final combined version: CBAM attention (v7) + BatchNorm (v8) + Perceptual Loss (v10). Designed to be the strongest overall model.'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey differences from v1_baseline (3 techniques combined):\")\n",
    "    print(\"  [v7]  CBAM: Channel + Spatial attention after every block\")\n",
    "    print(\"  [v8]  BatchNorm: After every convolution for training stability\")\n",
    "    print(\"  [v10] Perceptual Loss: VGG19 feature matching for texture preservation\")\n",
    "    print(\"  Block order: Conv → BN → Act → Conv → BN → CBAM → Residual\")\n",
    "    print(\"  Expected: Best overall SR quality combining all individual improvements\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd674b3e-dec6-473f-b012-e014807c2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script - v12_combined (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "v12 combines: CBAM Attention (v7) + Batch Normalization (v8) + Perceptual Loss (v10, training only)\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_v12_combined import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v12_combined',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v12'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - v12_combined (CBAM + BatchNorm)\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(out))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        return out * self.spatial_attention(out)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"ResidualBlock with both BatchNorm and CBAM\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.attention(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"RecursiveBlock with both BatchNorm and CBAM\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.activation(self.bn2(self.conv2(out)))\n",
    "        out = self.attention(out)\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(ch))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version='v12_combined', data_dir='./preprocessed_data', \n",
    "                   model_dir='./trained_models_v12',\n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels + CBAM + BatchNorm\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels + CBAM + BatchNorm\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\")\n",
    "    print(f\"  Attention: {config.get('use_attention', True)}\")\n",
    "    print(f\"  Batch Norm: {config.get('use_batch_norm', True)}\")\n",
    "    print(f\"  Perceptual Loss (training only): L1={config.get('l1_weight', 1.0)}, Perceptual={config.get('perceptual_weight', 0.1)}\\n\")\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    lapsrn = LapSRN().to(device)\n",
    "    drrn = DRRN().to(device)\n",
    "    classifier = MedicalImageClassifier().to(device)\n",
    "    \n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list, lapsrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                lapsrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                lapsrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list, drrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                drrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                drrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds, all_labels, all_urgency_preds, all_urgency_true = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'], output_dict=True)\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': { 'psnr': float(lapsrn_psnr_mean), 'ssim': float(lapsrn_ssim_mean) },\n",
    "        'drrn': { 'psnr': float(drrn_psnr_mean), 'ssim': float(drrn_ssim_mean) },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019a74dc-0a97-4403-84d9-fd84978c8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING MODEL: v12_combined\n",
      "================================================================================\n",
      "\n",
      "Loaded configuration for v12_combined\n",
      "  LapSRN: 64 channels + CBAM + BatchNorm\n",
      "  DRRN: 128 channels + CBAM + BatchNorm\n",
      "  Backbone: resnet50\n",
      "  Attention: True\n",
      "  Batch Norm: True\n",
      "  Perceptual Loss (training only): L1=1.0, Perceptual=0.1\n",
      "\n",
      "Creating datasets...\n",
      "  SR dataset: 6636 samples\n",
      "  DRRN dataset: 6636 samples\n",
      "  Classification dataset: 6636 samples\n",
      "\n",
      "Loading models...\n",
      "✓ Models loaded successfully\n",
      "\n",
      "================================================================================\n",
      "[1/3] Evaluating LapSRN (16x16 → 64x64)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LapSRN Evaluation: 100% 415/415 [00:11<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ LapSRN Results:\n",
      "  PSNR: 32.4180 dB\n",
      "  SSIM: 0.7942\n",
      "\n",
      "================================================================================\n",
      "[2/3] Evaluating DRRN (64x64 → 128x128)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DRRN Evaluation: 100% 415/415 [00:11<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DRRN Results:\n",
      "  PSNR: 40.8093 dB\n",
      "  SSIM: 0.9812\n",
      "\n",
      "================================================================================\n",
      "[3/3] Evaluating Classifier\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Evaluation: 100% 415/415 [00:14<00:00, 29.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification Results:\n",
      "  Accuracy: 97.29%\n",
      "\n",
      "  Confusion Matrix:\n",
      "  [[4413   13    1]\n",
      " [  66 1050    0]\n",
      " [  96    4  993]]\n",
      "\n",
      "  Per-Class Metrics:\n",
      "    Normal:\n",
      "      Precision: 0.9646\n",
      "      Recall: 0.9968\n",
      "      F1-Score: 0.9804\n",
      "    Ischemia:\n",
      "      Precision: 0.9841\n",
      "      Recall: 0.9409\n",
      "      F1-Score: 0.9620\n",
      "    Bleeding:\n",
      "      Precision: 0.9990\n",
      "      Recall: 0.9085\n",
      "      F1-Score: 0.9516\n",
      "\n",
      "  Urgency Prediction:\n",
      "    MSE: 0.0120\n",
      "    MAE: 0.0402\n",
      "\n",
      "✓ Results saved to: ./trained_models_v12/v12_combined/evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE FOR v12_combined\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(version='v12_combined', data_dir='./preprocessed_data', model_dir='./trained_models_v12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5401a-984b-4990-8e7b-b78a789ef905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
