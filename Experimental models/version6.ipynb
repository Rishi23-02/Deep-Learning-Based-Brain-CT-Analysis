{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143809f5-4b6c-4527-8e49-cc0ab32243a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in /opt/venv/lib/python3.10/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/venv/lib/python3.10/site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in /opt/venv/lib/python3.10/site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in /opt/venv/lib/python3.10/site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /opt/venv/lib/python3.10/site-packages (from kaggle) (5.29.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/venv/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in /opt/venv/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/venv/lib/python3.10/site-packages (from kaggle) (69.5.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/venv/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/venv/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/venv/lib/python3.10/site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in /opt/venv/lib/python3.10/site-packages (from kaggle) (0.5.1)\n",
      "Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [kaggle]\n",
      "\u001b[1A\u001b[2KSuccessfully installed kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c93199-da98-4f8d-a97f-ab3bd0844aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set and permissions updated!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "!chmod 600 kaggle.json\n",
    "print(\"Config set and permissions updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8e2219-0649-4c24-a022-5d8364e7162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                               title                    size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "--------------------------------  -----------------  ----------  -------------------  -------------  ---------  ---------------  \n",
      "rokkamrishitha/preprocessed-data  preprocessed_data   552614772  2026-01-30 06:21:40              5          0  0.0              \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d941ae3-d24c-4367-bc5d-c9cce9715494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rokkamrishitha/preprocessed-data\n",
      "License(s): unknown\n",
      "Downloading preprocessed-data.zip to .\n",
      " 88%|███████████████████████████████████▏    | 464M/527M [00:00<00:00, 2.43GB/s]\n",
      "100%|████████████████████████████████████████| 527M/527M [00:00<00:00, 2.41GB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rokkamrishitha/preprocessed-data --unzip -p ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4181435-6032-47ab-bc62-38e5b4c83d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydicom\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Collecting SimpleITK\n",
      "  Downloading simpleitk-2.5.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simpleitk-2.5.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (52.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: SimpleITK, pydicom\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pydicom]m1/2\u001b[0m [pydicom]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SimpleITK-2.5.3 pydicom-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b137d88-8ce8-40d0-8329-734ff30fa7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V6_RESNET34_BACKBONE\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET34 (lighter than ResNet50)\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 83.3M/83.3M [00:00<00:00, 332MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier backbone info:\n",
      "  - Architecture: ResNet34 (21.8M parameters)\n",
      "  - Feature dimension: 512 (vs 2048 in ResNet50)\n",
      "  - Advantage: Faster training, less overfitting\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [00:43<00:00, 30.19it/s, loss=0.009210]\n",
      "Epoch 2/50: 100% 1328/1328 [00:36<00:00, 36.34it/s, loss=0.010131]\n",
      "Epoch 3/50: 100% 1328/1328 [00:36<00:00, 36.33it/s, loss=0.011959]\n",
      "Epoch 4/50: 100% 1328/1328 [00:36<00:00, 36.45it/s, loss=0.009980]\n",
      "Epoch 5/50: 100% 1328/1328 [00:36<00:00, 36.42it/s, loss=0.031309]\n",
      "Epoch 6/50: 100% 1328/1328 [00:36<00:00, 36.50it/s, loss=0.007950]\n",
      "Epoch 7/50: 100% 1328/1328 [00:36<00:00, 36.55it/s, loss=0.002126]\n",
      "Epoch 8/50: 100% 1328/1328 [00:36<00:00, 36.54it/s, loss=0.011048]\n",
      "Epoch 9/50: 100% 1328/1328 [00:36<00:00, 36.28it/s, loss=0.021833]\n",
      "Epoch 10/50: 100% 1328/1328 [00:36<00:00, 36.09it/s, loss=0.000408]\n",
      "Epoch 11/50: 100% 1328/1328 [00:36<00:00, 35.98it/s, loss=0.000154]\n",
      "Epoch 12/50: 100% 1328/1328 [00:36<00:00, 36.29it/s, loss=0.011516]\n",
      "Epoch 13/50: 100% 1328/1328 [00:36<00:00, 36.06it/s, loss=0.021391]\n",
      "Epoch 14/50: 100% 1328/1328 [00:36<00:00, 36.13it/s, loss=0.003899]\n",
      "Epoch 15/50: 100% 1328/1328 [00:36<00:00, 36.18it/s, loss=0.008194]\n",
      "Epoch 16/50: 100% 1328/1328 [00:36<00:00, 36.22it/s, loss=0.000080]\n",
      "Epoch 17/50: 100% 1328/1328 [00:36<00:00, 36.05it/s, loss=0.016243]\n",
      "Epoch 18/50: 100% 1328/1328 [00:36<00:00, 36.41it/s, loss=0.011125]\n",
      "Epoch 19/50: 100% 1328/1328 [00:36<00:00, 36.04it/s, loss=0.011714]\n",
      "Epoch 20/50: 100% 1328/1328 [00:37<00:00, 35.59it/s, loss=0.002462]\n",
      "Epoch 21/50: 100% 1328/1328 [00:37<00:00, 35.83it/s, loss=0.006150]\n",
      "Epoch 22/50: 100% 1328/1328 [00:37<00:00, 35.42it/s, loss=0.007903]\n",
      "Epoch 23/50: 100% 1328/1328 [00:37<00:00, 35.48it/s, loss=0.002815]\n",
      "Epoch 24/50: 100% 1328/1328 [00:37<00:00, 35.87it/s, loss=0.009345]\n",
      "Epoch 25/50: 100% 1328/1328 [00:37<00:00, 35.67it/s, loss=0.011984]\n",
      "Epoch 26/50: 100% 1328/1328 [00:38<00:00, 34.74it/s, loss=0.001121]\n",
      "Epoch 27/50: 100% 1328/1328 [00:37<00:00, 34.96it/s, loss=0.007034]\n",
      "Epoch 28/50: 100% 1328/1328 [00:37<00:00, 35.13it/s, loss=0.000116]\n",
      "Epoch 29/50: 100% 1328/1328 [00:37<00:00, 35.18it/s, loss=0.006851]\n",
      "Epoch 30/50: 100% 1328/1328 [00:37<00:00, 34.98it/s, loss=0.008445]\n",
      "Epoch 31/50: 100% 1328/1328 [00:37<00:00, 35.20it/s, loss=0.000041]\n",
      "Epoch 32/50: 100% 1328/1328 [00:37<00:00, 34.97it/s, loss=0.018174]\n",
      "Epoch 33/50: 100% 1328/1328 [00:37<00:00, 34.97it/s, loss=0.008040]\n",
      "Epoch 34/50: 100% 1328/1328 [00:37<00:00, 35.22it/s, loss=0.031739]\n",
      "Epoch 35/50: 100% 1328/1328 [00:37<00:00, 35.25it/s, loss=0.021516]\n",
      "Epoch 38/50: 100% 1328/1328 [00:37<00:00, 35.15it/s, loss=0.017064]\n",
      "Epoch 39/50: 100% 1328/1328 [00:37<00:00, 35.09it/s, loss=0.009994]\n",
      "Epoch 40/50: 100% 1328/1328 [00:37<00:00, 34.95it/s, loss=0.005468]\n",
      "Epoch 41/50: 100% 1328/1328 [00:38<00:00, 34.88it/s, loss=0.003932]\n",
      "Epoch 42/50: 100% 1328/1328 [00:38<00:00, 34.51it/s, loss=0.024800]\n",
      "Epoch 43/50: 100% 1328/1328 [00:38<00:00, 34.44it/s, loss=0.005083]\n",
      "Epoch 44/50: 100% 1328/1328 [00:37<00:00, 35.04it/s, loss=0.000813]\n",
      "Epoch 45/50: 100% 1328/1328 [00:38<00:00, 34.64it/s, loss=0.009360]\n",
      "Epoch 46/50: 100% 1328/1328 [00:38<00:00, 34.07it/s, loss=0.000068]\n",
      "Epoch 47/50: 100% 1328/1328 [00:38<00:00, 34.45it/s, loss=0.003452]\n",
      "Epoch 48/50: 100% 1328/1328 [00:38<00:00, 34.43it/s, loss=0.014238]\n",
      "Epoch 49/50: 100% 1328/1328 [00:38<00:00, 34.63it/s, loss=0.006283]\n",
      "Epoch 50/50: 100% 1328/1328 [00:38<00:00, 34.89it/s, loss=0.003760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.009613)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128, 2x upsampling)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [00:41<00:00, 32.04it/s, loss=0.004233]\n",
      "Epoch 2/50: 100% 1328/1328 [00:39<00:00, 33.75it/s, loss=0.005814]\n",
      "Epoch 3/50: 100% 1328/1328 [00:39<00:00, 33.91it/s, loss=0.003085]\n",
      "Epoch 4/50: 100% 1328/1328 [00:38<00:00, 34.17it/s, loss=0.005556]\n",
      "Epoch 5/50: 100% 1328/1328 [00:38<00:00, 34.19it/s, loss=0.006232]\n",
      "Epoch 6/50: 100% 1328/1328 [00:39<00:00, 33.83it/s, loss=0.001902]\n",
      "Epoch 7/50: 100% 1328/1328 [00:39<00:00, 33.77it/s, loss=0.003521]\n",
      "Epoch 8/50: 100% 1328/1328 [00:39<00:00, 33.84it/s, loss=0.004324]\n",
      "Epoch 9/50: 100% 1328/1328 [00:39<00:00, 34.03it/s, loss=0.004586]\n",
      "Epoch 10/50: 100% 1328/1328 [00:39<00:00, 34.03it/s, loss=0.000350]\n",
      "Epoch 11/50: 100% 1328/1328 [00:38<00:00, 34.12it/s, loss=0.000562]\n",
      "Epoch 12/50: 100% 1328/1328 [00:38<00:00, 34.09it/s, loss=0.002074]\n",
      "Epoch 13/50: 100% 1328/1328 [00:38<00:00, 34.22it/s, loss=0.003187]\n",
      "Epoch 14/50: 100% 1328/1328 [00:38<00:00, 34.09it/s, loss=0.002729]\n",
      "Epoch 15/50: 100% 1328/1328 [00:39<00:00, 33.67it/s, loss=0.002906]\n",
      "Epoch 16/50: 100% 1328/1328 [00:39<00:00, 33.36it/s, loss=0.000427]\n",
      "Epoch 17/50: 100% 1328/1328 [00:42<00:00, 31.26it/s, loss=0.001104]\n",
      "Epoch 18/50: 100% 1328/1328 [00:40<00:00, 32.84it/s, loss=0.004892]\n",
      "Epoch 19/50: 100% 1328/1328 [00:43<00:00, 30.73it/s, loss=0.003075]\n",
      "Epoch 20/50: 100% 1328/1328 [00:57<00:00, 23.23it/s, loss=0.000626]\n",
      "Epoch 21/50: 100% 1328/1328 [00:57<00:00, 23.18it/s, loss=0.001505]\n",
      "Epoch 22/50: 100% 1328/1328 [00:58<00:00, 22.78it/s, loss=0.000327]\n",
      "Epoch 23/50: 100% 1328/1328 [00:59<00:00, 22.33it/s, loss=0.000914]\n",
      "Epoch 24/50: 100% 1328/1328 [01:12<00:00, 18.40it/s, loss=0.004492]\n",
      "Epoch 25/50: 100% 1328/1328 [01:11<00:00, 18.56it/s, loss=0.003691]\n",
      "Epoch 26/50: 100% 1328/1328 [01:11<00:00, 18.48it/s, loss=0.003545]\n",
      "Epoch 27/50: 100% 1328/1328 [01:11<00:00, 18.47it/s, loss=0.001119]\n",
      "Epoch 28/50: 100% 1328/1328 [01:12<00:00, 18.39it/s, loss=0.002128]\n",
      "Epoch 29/50: 100% 1328/1328 [01:12<00:00, 18.37it/s, loss=0.002063]\n",
      "Epoch 30/50: 100% 1328/1328 [01:12<00:00, 18.42it/s, loss=0.005045]\n",
      "Epoch 31/50: 100% 1328/1328 [01:12<00:00, 18.41it/s, loss=0.000338]\n",
      "Epoch 32/50: 100% 1328/1328 [01:12<00:00, 18.42it/s, loss=0.001401]\n",
      "Epoch 33/50: 100% 1328/1328 [01:12<00:00, 18.38it/s, loss=0.000788]\n",
      "Epoch 34/50: 100% 1328/1328 [01:11<00:00, 18.46it/s, loss=0.004569]\n",
      "Epoch 35/50: 100% 1328/1328 [01:12<00:00, 18.43it/s, loss=0.003728]\n",
      "Epoch 36/50: 100% 1328/1328 [01:11<00:00, 18.45it/s, loss=0.000452]\n",
      "Epoch 37/50: 100% 1328/1328 [01:12<00:00, 18.40it/s, loss=0.001324]\n",
      "Epoch 38/50: 100% 1328/1328 [01:12<00:00, 18.36it/s, loss=0.002826]\n",
      "Epoch 39/50: 100% 1328/1328 [01:12<00:00, 18.41it/s, loss=0.002259]\n",
      "Epoch 40/50: 100% 1328/1328 [01:11<00:00, 18.45it/s, loss=0.002365]\n",
      "Epoch 41/50: 100% 1328/1328 [01:11<00:00, 18.45it/s, loss=0.000314]\n",
      "Epoch 42/50: 100% 1328/1328 [01:12<00:00, 18.41it/s, loss=0.001135]\n",
      "Epoch 43/50: 100% 1328/1328 [01:12<00:00, 18.33it/s, loss=0.000127]\n",
      "Epoch 44/50: 100% 1328/1328 [01:12<00:00, 18.41it/s, loss=0.002339]\n",
      "Epoch 45/50: 100% 1328/1328 [01:12<00:00, 18.41it/s, loss=0.004731]\n",
      "Epoch 46/50: 100% 1328/1328 [01:12<00:00, 18.44it/s, loss=0.001215]\n",
      "Epoch 47/50: 100% 1328/1328 [01:12<00:00, 18.36it/s, loss=0.001431]\n",
      "Epoch 48/50: 100% 1328/1328 [01:12<00:00, 18.37it/s, loss=0.002509]\n",
      "Epoch 49/50: 100% 1328/1328 [01:12<00:00, 18.36it/s, loss=0.005141]\n",
      "Epoch 50/50: 100% 1328/1328 [01:12<00:00, 18.44it/s, loss=0.004620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.002359)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0% 0/332 [00:00<?, ?it/s]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x1x1x1x128x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x1x1x1x128x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x1x1x1x128x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x1x1x1x128x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x1x1x1x128x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x1x1x1x64x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x1x1x1x64x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x1x1x1x64x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x1x1x1x64x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x1x1x1x64x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x64x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x64x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x64x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x64x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x64x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 331/332 [00:25<00:00, 24.75it/s, acc=79.46%]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x1x1x1x128x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x1x1x1x128x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x1x1x1x64x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x1x1x1x64x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x1x1x1x64x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x1x1x1x64x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x1x1x1x64x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x64x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x64x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x64x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x64x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x64x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 332/332 [00:27<00:00, 12.22it/s, acc=79.48%]\n",
      "Epoch 2/30: 100% 332/332 [00:14<00:00, 23.39it/s, acc=87.66%]\n",
      "Epoch 3/30: 100% 332/332 [00:14<00:00, 23.58it/s, acc=91.94%]\n",
      "Epoch 4/30: 100% 332/332 [00:14<00:00, 23.58it/s, acc=93.97%]\n",
      "Epoch 5/30: 100% 332/332 [00:14<00:00, 23.53it/s, acc=94.97%]\n",
      "Epoch 6/30: 100% 332/332 [00:13<00:00, 23.87it/s, acc=95.21%]\n",
      "Epoch 7/30: 100% 332/332 [00:13<00:00, 23.91it/s, acc=95.95%]\n",
      "Epoch 8/30: 100% 332/332 [00:13<00:00, 23.90it/s, acc=96.36%]\n",
      "Epoch 9/30: 100% 332/332 [00:14<00:00, 23.33it/s, acc=96.76%]\n",
      "Epoch 10/30: 100% 332/332 [00:14<00:00, 23.51it/s, acc=96.82%]\n",
      "Epoch 11/30: 100% 332/332 [00:14<00:00, 23.48it/s, acc=97.59%]\n",
      "Epoch 12/30: 100% 332/332 [00:14<00:00, 23.57it/s, acc=96.74%]\n",
      "Epoch 13/30: 100% 332/332 [00:14<00:00, 23.35it/s, acc=97.08%]\n",
      "Epoch 14/30: 100% 332/332 [00:13<00:00, 24.00it/s, acc=97.70%]\n",
      "Epoch 15/30: 100% 332/332 [00:14<00:00, 23.55it/s, acc=97.53%]\n",
      "Epoch 16/30: 100% 332/332 [00:13<00:00, 23.73it/s, acc=96.48%]\n",
      "Epoch 17/30: 100% 332/332 [00:13<00:00, 23.74it/s, acc=97.93%]\n",
      "Epoch 18/30: 100% 332/332 [00:13<00:00, 23.76it/s, acc=97.12%]\n",
      "Epoch 19/30: 100% 332/332 [00:14<00:00, 23.61it/s, acc=97.70%]\n",
      "Epoch 20/30: 100% 332/332 [00:14<00:00, 23.59it/s, acc=97.87%]\n",
      "Epoch 21/30: 100% 332/332 [00:13<00:00, 23.75it/s, acc=97.23%]\n",
      "Epoch 22/30: 100% 332/332 [00:14<00:00, 23.59it/s, acc=97.87%]\n",
      "Epoch 23/30: 100% 332/332 [00:13<00:00, 23.73it/s, acc=97.70%]\n",
      "Epoch 24/30: 100% 332/332 [00:13<00:00, 23.89it/s, acc=97.63%]\n",
      "Epoch 25/30: 100% 332/332 [00:13<00:00, 23.81it/s, acc=97.85%]\n",
      "Epoch 26/30: 100% 332/332 [00:14<00:00, 23.58it/s, acc=97.89%]\n",
      "Epoch 27/30: 100% 332/332 [00:14<00:00, 23.67it/s, acc=97.25%]\n",
      "Epoch 28/30: 100% 332/332 [00:14<00:00, 23.71it/s, acc=98.15%]\n",
      "Epoch 29/30: 100% 332/332 [00:14<00:00, 23.60it/s, acc=98.29%]\n",
      "Epoch 30/30: 100% 332/332 [00:13<00:00, 23.90it/s, acc=98.04%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classifier training complete (best accuracy: 98.29%)\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Models saved to: ./trained_models_v6/v6_resnet34_backbone\n",
      "\n",
      "Pipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\n",
      "\n",
      "Key difference from v1_baseline:\n",
      "  - Classifier backbone: ResNet50 → ResNet34\n",
      "  - Feature dimension: 2048 → 512\n",
      "  - Parameters: ~44M → ~22M (50% reduction)\n",
      "  - Expected: Faster training, less overfitting, slightly lower capacity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v6_resnet34_backbone (ResNet34 Backbone)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | ResNet34\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v6_resnet34_backbone'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v6'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v6_resnet34_backbone specific - USING ResNet34 INSTEAD OF ResNet50\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet34'  # Changed from 'resnet50'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BUILDING BLOCKS\n",
    "# ==============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v6_resnet34_backbone\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v6_resnet34_backbone: Standard LapSRN (same as baseline)\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v6_resnet34_backbone: Standard DRRN (same as baseline)\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v6_resnet34_backbone: ResNet34 backbone classifier (lighter than ResNet50)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        # USING ResNet34 INSTEAD OF ResNet50\n",
    "        self.backbone = models.resnet34(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features  # 512 for ResNet34 vs 2048 for ResNet50\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()} (lighter than ResNet50)\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN().to(config.DEVICE)\n",
    "    drrn = DRRN().to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    print(f\"\\nClassifier backbone info:\")\n",
    "    print(f\"  - Architecture: ResNet34 (21.8M parameters)\")\n",
    "    print(f\"  - Feature dimension: 512 (vs 2048 in ResNet50)\")\n",
    "    print(f\"  - Advantage: Faster training, less overfitting\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.L1Loss()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128, 2x upsampling)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Using ResNet34 instead of ResNet50 for lighter classifier'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey difference from v1_baseline:\")\n",
    "    print(\"  - Classifier backbone: ResNet50 → ResNet34\")\n",
    "    print(\"  - Feature dimension: 2048 → 512\")\n",
    "    print(\"  - Parameters: ~44M → ~22M (50% reduction)\")\n",
    "    print(\"  - Expected: Faster training, less overfitting, slightly lower capacity\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4bd754-d52b-4f65-88ef-006de9e690fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Model Evaluation Script for All Versions (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_models_complete import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v6_resnet34_backbone',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v6'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        # Center crop\n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        # Center crop\n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - ALL VERSIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Attention modules for v7\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(out))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        return out * self.spatial_attention(out)\n",
    "\n",
    "\n",
    "# Building blocks\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=False, use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn2(out)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            out = self.attention(out)\n",
    "        \n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=False, use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            out = self.attention(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# LapSRN\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1, use_attention=False, use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3, use_attention, use_batch_norm))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(ch))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "# DRRN\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2, use_attention=False, use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3, use_attention, use_batch_norm))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "# Classifier\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, backbone='resnet50'):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        if backbone == 'resnet34':\n",
    "            self.backbone = models.resnet34(pretrained=True)\n",
    "            num_features = 512\n",
    "        else:\n",
    "            self.backbone = models.resnet50(pretrained=True)\n",
    "            num_features = 2048\n",
    "        \n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version, data_dir='./preprocessed_data', model_dir='./trained_models_v6', \n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model version\n",
    "    \n",
    "    Args:\n",
    "        version: Version name (e.g., 'v6_resnet34_backbone')\n",
    "        data_dir: Path to preprocessed data\n",
    "        model_dir: Base directory containing trained models\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load configuration\n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels, {config.get('lapsrn_blocks', 5)} blocks\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels, {config.get('drrn_blocks', 25)} blocks\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\")\n",
    "    print(f\"  Attention: {config.get('use_attention', False)}\")\n",
    "    print(f\"  Batch Norm: {config.get('use_batch_norm', False)}\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"Loading models...\")\n",
    "    use_attention = config.get('use_attention', False)\n",
    "    use_batch_norm = config.get('use_batch_norm', False)\n",
    "    backbone = config.get('backbone', 'resnet50')\n",
    "    \n",
    "    lapsrn = LapSRN(use_attention=use_attention, use_batch_norm=use_batch_norm).to(device)\n",
    "    drrn = DRRN(use_attention=use_attention, use_batch_norm=use_batch_norm).to(device)\n",
    "    classifier = MedicalImageClassifier(backbone=backbone).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list = []\n",
    "    lapsrn_ssim_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                \n",
    "                psnr_val = psnr(hr_img, sr_img, data_range=1.0)\n",
    "                ssim_val = ssim(hr_img, sr_img, data_range=1.0)\n",
    "                \n",
    "                lapsrn_psnr_list.append(psnr_val)\n",
    "                lapsrn_ssim_list.append(ssim_val)\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list = []\n",
    "    drrn_ssim_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                \n",
    "                psnr_val = psnr(hr_img, sr_img, data_range=1.0)\n",
    "                ssim_val = ssim(hr_img, sr_img, data_range=1.0)\n",
    "                \n",
    "                drrn_psnr_list.append(psnr_val)\n",
    "                drrn_ssim_list.append(ssim_val)\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_urgency_preds = []\n",
    "    all_urgency_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    # Classification metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'],\n",
    "                                        output_dict=True)\n",
    "    \n",
    "    # Urgency metrics\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': {\n",
    "            'psnr': float(lapsrn_psnr_mean),\n",
    "            'ssim': float(lapsrn_ssim_mean)\n",
    "        },\n",
    "        'drrn': {\n",
    "            'psnr': float(drrn_psnr_mean),\n",
    "            'ssim': float(drrn_ssim_mean)\n",
    "        },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193b858c-5629-475a-af08-b4aac6045442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING MODEL: v6_resnet34_backbone\n",
      "================================================================================\n",
      "\n",
      "Loaded configuration for v6_resnet34_backbone\n",
      "  LapSRN: 64 channels, 5 blocks\n",
      "  DRRN: 128 channels, 25 blocks\n",
      "  Backbone: resnet34\n",
      "  Attention: False\n",
      "  Batch Norm: False\n",
      "\n",
      "Creating datasets...\n",
      "  SR dataset: 6636 samples\n",
      "  DRRN dataset: 6636 samples\n",
      "  Classification dataset: 6636 samples\n",
      "\n",
      "Loading models...\n",
      "✓ Models loaded successfully\n",
      "\n",
      "================================================================================\n",
      "[1/3] Evaluating LapSRN (16x16 → 64x64)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LapSRN Evaluation: 100% 415/415 [00:12<00:00, 32.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ LapSRN Results:\n",
      "  PSNR: 32.5820 dB\n",
      "  SSIM: 0.8232\n",
      "\n",
      "================================================================================\n",
      "[2/3] Evaluating DRRN (64x64 → 128x128)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DRRN Evaluation: 100% 415/415 [00:12<00:00, 32.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DRRN Results:\n",
      "  PSNR: 44.0735 dB\n",
      "  SSIM: 0.9839\n",
      "\n",
      "================================================================================\n",
      "[3/3] Evaluating Classifier\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Evaluation: 100% 415/415 [00:14<00:00, 27.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification Results:\n",
      "  Accuracy: 97.18%\n",
      "\n",
      "  Confusion Matrix:\n",
      "  [[4414    8    5]\n",
      " [  67 1048    1]\n",
      " [ 101    5  987]]\n",
      "\n",
      "  Per-Class Metrics:\n",
      "    Normal:\n",
      "      Precision: 0.9633\n",
      "      Recall: 0.9971\n",
      "      F1-Score: 0.9799\n",
      "    Ischemia:\n",
      "      Precision: 0.9877\n",
      "      Recall: 0.9391\n",
      "      F1-Score: 0.9628\n",
      "    Bleeding:\n",
      "      Precision: 0.9940\n",
      "      Recall: 0.9030\n",
      "      F1-Score: 0.9463\n",
      "\n",
      "  Urgency Prediction:\n",
      "    MSE: 0.0121\n",
      "    MAE: 0.0417\n",
      "\n",
      "✓ Results saved to: ./trained_models_v6/v6_resnet34_backbone/evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE FOR v6_resnet34_backbone\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model('v6_resnet34_backbone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8686c-1a10-48a3-99c9-f21f1846ac2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
