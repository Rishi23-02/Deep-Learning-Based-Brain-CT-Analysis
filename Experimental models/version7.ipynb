{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e502b43-369e-4f9c-9b39-90b783772050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: SimpleITK in /opt/venv/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafeeead-7efd-4685-a443-a4300e4c115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V7_ATTENTION\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET50\n",
      "  - Attention: CBAM (Channel + Spatial)\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Attention mechanism info:\n",
      "  - Type: CBAM (Convolutional Block Attention Module)\n",
      "  - Channel Attention: Adaptive avg/max pooling + FC layers\n",
      "  - Spatial Attention: 7x7 convolution on channel-pooled features\n",
      "  - Applied after: Every residual/recursive block\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + CBAM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [00:40<00:00, 32.90it/s, loss=0.024421]\n",
      "Epoch 2/50: 100% 1328/1328 [00:36<00:00, 36.02it/s, loss=0.009047]\n",
      "Epoch 3/50: 100% 1328/1328 [00:36<00:00, 36.02it/s, loss=0.014735]\n",
      "Epoch 4/50: 100% 1328/1328 [00:36<00:00, 36.19it/s, loss=0.013262]\n",
      "Epoch 5/50: 100% 1328/1328 [00:36<00:00, 36.26it/s, loss=0.000074]\n",
      "Epoch 6/50: 100% 1328/1328 [00:36<00:00, 36.24it/s, loss=0.000500]\n",
      "Epoch 7/50: 100% 1328/1328 [00:37<00:00, 35.88it/s, loss=0.001071]\n",
      "Epoch 8/50: 100% 1328/1328 [00:36<00:00, 35.97it/s, loss=0.007731]\n",
      "Epoch 9/50: 100% 1328/1328 [00:36<00:00, 36.28it/s, loss=0.018475]\n",
      "Epoch 10/50: 100% 1328/1328 [00:36<00:00, 35.99it/s, loss=0.022867]\n",
      "Epoch 11/50: 100% 1328/1328 [00:37<00:00, 35.72it/s, loss=0.011397]\n",
      "Epoch 12/50: 100% 1328/1328 [00:37<00:00, 35.50it/s, loss=0.008589]\n",
      "Epoch 13/50: 100% 1328/1328 [00:37<00:00, 35.50it/s, loss=0.005311]\n",
      "Epoch 14/50: 100% 1328/1328 [00:37<00:00, 35.27it/s, loss=0.012764]\n",
      "Epoch 15/50: 100% 1328/1328 [00:37<00:00, 35.38it/s, loss=0.013430]\n",
      "Epoch 16/50: 100% 1328/1328 [00:37<00:00, 35.04it/s, loss=0.006931]\n",
      "Epoch 17/50: 100% 1328/1328 [00:38<00:00, 34.22it/s, loss=0.001440]\n",
      "Epoch 18/50: 100% 1328/1328 [00:42<00:00, 31.27it/s, loss=0.007811]\n",
      "Epoch 19/50: 100% 1328/1328 [00:41<00:00, 32.20it/s, loss=0.016894]\n",
      "Epoch 20/50: 100% 1328/1328 [00:38<00:00, 34.68it/s, loss=0.009056]\n",
      "Epoch 21/50: 100% 1328/1328 [00:38<00:00, 34.44it/s, loss=0.009507]\n",
      "Epoch 22/50: 100% 1328/1328 [00:38<00:00, 34.57it/s, loss=0.019715]\n",
      "Epoch 23/50: 100% 1328/1328 [00:38<00:00, 34.69it/s, loss=0.001330]\n",
      "Epoch 24/50: 100% 1328/1328 [00:38<00:00, 34.45it/s, loss=0.006823]\n",
      "Epoch 25/50: 100% 1328/1328 [00:39<00:00, 34.05it/s, loss=0.002401]\n",
      "Epoch 26/50: 100% 1328/1328 [00:38<00:00, 34.10it/s, loss=0.014213]\n",
      "Epoch 27/50: 100% 1328/1328 [00:39<00:00, 33.67it/s, loss=0.013834]\n",
      "Epoch 28/50: 100% 1328/1328 [00:39<00:00, 33.93it/s, loss=0.005925]\n",
      "Epoch 29/50: 100% 1328/1328 [00:54<00:00, 24.30it/s, loss=0.000035]\n",
      "Epoch 30/50: 100% 1328/1328 [01:05<00:00, 20.32it/s, loss=0.018969]\n",
      "Epoch 31/50: 100% 1328/1328 [01:06<00:00, 19.92it/s, loss=0.013912]\n",
      "Epoch 32/50: 100% 1328/1328 [01:06<00:00, 19.94it/s, loss=0.018727]\n",
      "Epoch 33/50: 100% 1328/1328 [01:07<00:00, 19.81it/s, loss=0.011897]\n",
      "Epoch 34/50: 100% 1328/1328 [01:05<00:00, 20.17it/s, loss=0.002439]\n",
      "Epoch 35/50: 100% 1328/1328 [01:05<00:00, 20.26it/s, loss=0.019524]\n",
      "Epoch 36/50: 100% 1328/1328 [01:06<00:00, 19.99it/s, loss=0.004943]\n",
      "Epoch 37/50: 100% 1328/1328 [01:06<00:00, 20.04it/s, loss=0.000097]\n",
      "Epoch 38/50: 100% 1328/1328 [01:06<00:00, 19.98it/s, loss=0.010297]\n",
      "Epoch 39/50: 100% 1328/1328 [01:05<00:00, 20.26it/s, loss=0.014022]\n",
      "Epoch 40/50: 100% 1328/1328 [01:05<00:00, 20.21it/s, loss=0.006914]\n",
      "Epoch 41/50: 100% 1328/1328 [01:06<00:00, 19.98it/s, loss=0.022357]\n",
      "Epoch 42/50: 100% 1328/1328 [01:05<00:00, 20.20it/s, loss=0.008006]\n",
      "Epoch 43/50: 100% 1328/1328 [01:05<00:00, 20.38it/s, loss=0.001006]\n",
      "Epoch 44/50: 100% 1328/1328 [01:06<00:00, 20.09it/s, loss=0.008442]\n",
      "Epoch 45/50: 100% 1328/1328 [01:05<00:00, 20.31it/s, loss=0.014202]\n",
      "Epoch 46/50: 100% 1328/1328 [01:05<00:00, 20.20it/s, loss=0.010442]\n",
      "Epoch 47/50: 100% 1328/1328 [01:06<00:00, 20.02it/s, loss=0.008536]\n",
      "Epoch 48/50: 100% 1328/1328 [01:22<00:00, 16.03it/s, loss=0.001224]\n",
      "Epoch 49/50: 100% 1328/1328 [01:31<00:00, 14.59it/s, loss=0.014573]\n",
      "Epoch 50/50: 100% 1328/1328 [01:30<00:00, 14.60it/s, loss=0.019539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.009958)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + CBAM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [04:12<00:00,  5.27it/s, loss=0.004676]\n",
      "Epoch 2/50: 100% 1328/1328 [04:43<00:00,  4.68it/s, loss=0.000093]\n",
      "Epoch 3/50: 100% 1328/1328 [04:45<00:00,  4.66it/s, loss=0.000213]\n",
      "Epoch 4/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.005010]\n",
      "Epoch 5/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.008265]\n",
      "Epoch 6/50: 100% 1328/1328 [04:45<00:00,  4.66it/s, loss=0.001057]\n",
      "Epoch 7/50: 100% 1328/1328 [04:44<00:00,  4.66it/s, loss=0.001676]\n",
      "Epoch 8/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.001375]\n",
      "Epoch 9/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.004319]\n",
      "Epoch 10/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.003074]\n",
      "Epoch 11/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.002373]\n",
      "Epoch 12/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.003004]\n",
      "Epoch 13/50: 100% 1328/1328 [04:45<00:00,  4.66it/s, loss=0.001025]\n",
      "Epoch 14/50: 100% 1328/1328 [04:45<00:00,  4.65it/s, loss=0.003516]\n",
      "Epoch 15/50: 100% 1328/1328 [04:44<00:00,  4.66it/s, loss=0.002491]\n",
      "Epoch 16/50: 100% 1328/1328 [04:32<00:00,  4.88it/s, loss=0.001085]\n",
      "Epoch 17/50: 100% 1328/1328 [04:37<00:00,  4.78it/s, loss=0.001731]\n",
      "Epoch 18/50: 100% 1328/1328 [04:05<00:00,  5.41it/s, loss=0.001195]\n",
      "Epoch 19/50: 100% 1328/1328 [03:45<00:00,  5.90it/s, loss=0.002100]\n",
      "Epoch 20/50: 100% 1328/1328 [03:20<00:00,  6.61it/s, loss=0.002590]\n",
      "Epoch 21/50: 100% 1328/1328 [03:10<00:00,  6.97it/s, loss=0.005268]\n",
      "Epoch 22/50: 100% 1328/1328 [02:56<00:00,  7.50it/s, loss=0.000765]\n",
      "Epoch 23/50: 100% 1328/1328 [02:36<00:00,  8.47it/s, loss=0.004487]\n",
      "Epoch 24/50: 100% 1328/1328 [02:15<00:00,  9.81it/s, loss=0.002755]\n",
      "Epoch 25/50: 100% 1328/1328 [01:52<00:00, 11.82it/s, loss=0.005227]\n",
      "Epoch 26/50: 100% 1328/1328 [01:22<00:00, 16.06it/s, loss=0.001746]\n",
      "Epoch 27/50: 100% 1328/1328 [01:07<00:00, 19.65it/s, loss=0.004348]\n",
      "Epoch 28/50: 100% 1328/1328 [01:07<00:00, 19.68it/s, loss=0.000260]\n",
      "Epoch 29/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.002943]\n",
      "Epoch 30/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.001489]\n",
      "Epoch 31/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.001796]\n",
      "Epoch 32/50: 100% 1328/1328 [01:07<00:00, 19.71it/s, loss=0.001326]\n",
      "Epoch 33/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.002254]\n",
      "Epoch 34/50: 100% 1328/1328 [01:07<00:00, 19.68it/s, loss=0.000055]\n",
      "Epoch 35/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.000129]\n",
      "Epoch 36/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.003724]\n",
      "Epoch 37/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.002324]\n",
      "Epoch 38/50: 100% 1328/1328 [01:07<00:00, 19.73it/s, loss=0.000066]\n",
      "Epoch 39/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.001956]\n",
      "Epoch 40/50: 100% 1328/1328 [01:07<00:00, 19.70it/s, loss=0.000942]\n",
      "Epoch 41/50: 100% 1328/1328 [01:07<00:00, 19.70it/s, loss=0.000121]\n",
      "Epoch 42/50: 100% 1328/1328 [01:07<00:00, 19.74it/s, loss=0.001642]\n",
      "Epoch 43/50: 100% 1328/1328 [01:07<00:00, 19.75it/s, loss=0.001705]\n",
      "Epoch 44/50: 100% 1328/1328 [01:07<00:00, 19.74it/s, loss=0.004435]\n",
      "Epoch 45/50: 100% 1328/1328 [01:07<00:00, 19.71it/s, loss=0.000658]\n",
      "Epoch 46/50: 100% 1328/1328 [01:07<00:00, 19.70it/s, loss=0.007328]\n",
      "Epoch 47/50: 100% 1328/1328 [01:07<00:00, 19.66it/s, loss=0.003698]\n",
      "Epoch 48/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.004723]\n",
      "Epoch 49/50: 100% 1328/1328 [01:07<00:00, 19.71it/s, loss=0.002251]\n",
      "Epoch 50/50: 100% 1328/1328 [01:07<00:00, 19.72it/s, loss=0.000075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.002384)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0% 0/332 [00:00<?, ?it/s]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30:  99% 330/332 [00:18<00:00, 30.07it/s, acc=78.19%]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 332/332 [00:18<00:00, 17.53it/s, acc=78.18%]\n",
      "Epoch 2/30: 100% 332/332 [00:11<00:00, 28.92it/s, acc=87.70%]\n",
      "Epoch 3/30: 100% 332/332 [00:11<00:00, 28.83it/s, acc=91.30%]\n",
      "Epoch 4/30: 100% 332/332 [00:11<00:00, 28.87it/s, acc=93.88%]\n",
      "Epoch 5/30: 100% 332/332 [00:11<00:00, 28.68it/s, acc=94.48%]\n",
      "Epoch 6/30:  38% 126/332 [00:04<00:07, 29.41it/s, acc=95.41%]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v7_attention (CBAM Attention Mechanisms)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | CBAM Attention\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v7_attention'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v7'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v7_attention specific - ADDING CBAM ATTENTION\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet50'\n",
    "    USE_ATTENTION = True  # NEW: Enable CBAM attention\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ATTENTION MODULES (CBAM)\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        out = out * self.spatial_attention(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BUILDING BLOCKS WITH ATTENTION\n",
    "# ==============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with CBAM attention\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            out = self.attention(out)\n",
    "        \n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"Recursive block with CBAM attention\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        \n",
    "        if self.use_attention:\n",
    "            out = self.attention(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v7_attention\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v7_attention: LapSRN with CBAM attention\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3, use_attention))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v7_attention: DRRN with CBAM attention\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3, use_attention))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v7_attention: Standard ResNet50 classifier (same as baseline)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()}\")\n",
    "    print(f\"  - Attention: CBAM (Channel + Spatial)\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN(use_attention=config.USE_ATTENTION).to(config.DEVICE)\n",
    "    drrn = DRRN(use_attention=config.USE_ATTENTION).to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    print(f\"\\nAttention mechanism info:\")\n",
    "    print(f\"  - Type: CBAM (Convolutional Block Attention Module)\")\n",
    "    print(f\"  - Channel Attention: Adaptive avg/max pooling + FC layers\")\n",
    "    print(f\"  - Spatial Attention: 7x7 convolution on channel-pooled features\")\n",
    "    print(f\"  - Applied after: Every residual/recursive block\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + CBAM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.L1Loss()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + CBAM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'use_attention': config.USE_ATTENTION,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Added CBAM attention (channel + spatial) after every residual/recursive block'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey difference from v1_baseline:\")\n",
    "    print(\"  - Added CBAM attention modules after each residual/recursive block\")\n",
    "    print(\"  - Channel attention: Learns which features to emphasize\")\n",
    "    print(\"  - Spatial attention: Learns where to focus spatially\")\n",
    "    print(\"  - Expected: Better feature selection, improved detail preservation\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423531b-d4ce-47d2-8804-2547963c4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script - v7_attention (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_v7_attention import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v7_attention',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v7'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - v7_attention\n",
    "# ==============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(out))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x * self.channel_attention(x)\n",
    "        return out * self.spatial_attention(out)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = self.attention(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, use_attention=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "        self.attention = CBAM(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        out = self.attention(out)\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            for _ in range(5):\n",
    "                layers.append(ResidualBlock(ch, 3, use_attention))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3, use_attention))\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version='v7_attention', data_dir='./preprocessed_data', \n",
    "                   model_dir='./trained_models_v7',\n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels, {config.get('lapsrn_blocks', 5)} blocks\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels, {config.get('drrn_blocks', 25)} blocks\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\")\n",
    "    print(f\"  Attention: {config.get('use_attention', True)}\\n\")\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    lapsrn = LapSRN(use_attention=True).to(device)\n",
    "    drrn = DRRN(use_attention=True).to(device)\n",
    "    classifier = MedicalImageClassifier().to(device)\n",
    "    \n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list, lapsrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                lapsrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                lapsrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list, drrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                drrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                drrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds, all_labels, all_urgency_preds, all_urgency_true = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'], output_dict=True)\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': { 'psnr': float(lapsrn_psnr_mean), 'ssim': float(lapsrn_ssim_mean) },\n",
    "        'drrn': { 'psnr': float(drrn_psnr_mean), 'ssim': float(drrn_ssim_mean) },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff4b90-f8c7-4a98-8642-4f49c1ee9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(version='v7_attention', data_dir='./preprocessed_data', model_dir='./trained_models_v7')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
