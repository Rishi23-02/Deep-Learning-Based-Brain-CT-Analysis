{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f13a06-868d-433a-8757-4cc56f21d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /opt/venv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in /opt/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /opt/venv/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: SimpleITK in /opt/venv/lib/python3.10/site-packages (2.5.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydicom numpy scikit-image pillow scipy SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f911dd-b77b-4e08-98d2-ffd4bb5ac22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING V9_DENSE_CONNECTIONS\n",
      "================================================================================\n",
      "Configuration:\n",
      "  - LapSRN: 64 channels, 5 blocks\n",
      "  - DRRN: 128 channels, 25 blocks\n",
      "  - Kernel: 3x3\n",
      "  - Activation: leaky\n",
      "  - Backbone: RESNET50\n",
      "  - Dense Connections: Enabled\n",
      "  - Device: cuda\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Dense Connection info:\n",
      "  - Type: DenseBlock (DenseNet-style connections)\n",
      "  - LapSRN: 2 DenseBlocks per pyramid level (growth_rate=16, num_layers=4)\n",
      "  - DRRN: 3 DenseBlocks at multi-scale collection points (growth_rate=32, num_layers=4)\n",
      "  - Each layer receives ALL previous layer outputs as input\n",
      "  - 1x1 conv compresses concatenated features back to original channels\n",
      "\n",
      "================================================================================\n",
      "[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Dense Blocks\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [00:41<00:00, 31.84it/s, loss=0.028415]\n",
      "Epoch 2/50: 100% 1328/1328 [00:37<00:00, 34.95it/s, loss=0.028384]\n",
      "Epoch 3/50: 100% 1328/1328 [00:37<00:00, 35.01it/s, loss=0.011137]\n",
      "Epoch 4/50: 100% 1328/1328 [00:37<00:00, 35.36it/s, loss=0.020175]\n",
      "Epoch 5/50: 100% 1328/1328 [00:37<00:00, 35.35it/s, loss=0.012303]\n",
      "Epoch 6/50: 100% 1328/1328 [00:38<00:00, 34.77it/s, loss=0.011186]\n",
      "Epoch 7/50: 100% 1328/1328 [00:38<00:00, 34.25it/s, loss=0.011265]\n",
      "Epoch 8/50: 100% 1328/1328 [00:42<00:00, 31.43it/s, loss=0.016119]\n",
      "Epoch 9/50: 100% 1328/1328 [00:40<00:00, 33.19it/s, loss=0.011880]\n",
      "Epoch 10/50: 100% 1328/1328 [00:38<00:00, 34.53it/s, loss=0.013129]\n",
      "Epoch 11/50: 100% 1328/1328 [00:38<00:00, 34.55it/s, loss=0.015241]\n",
      "Epoch 12/50: 100% 1328/1328 [00:38<00:00, 34.73it/s, loss=0.000089]\n",
      "Epoch 13/50: 100% 1328/1328 [00:38<00:00, 34.60it/s, loss=0.016882]\n",
      "Epoch 14/50: 100% 1328/1328 [00:38<00:00, 34.35it/s, loss=0.015540]\n",
      "Epoch 15/50: 100% 1328/1328 [00:39<00:00, 33.70it/s, loss=0.006740]\n",
      "Epoch 16/50: 100% 1328/1328 [00:38<00:00, 34.12it/s, loss=0.010353]\n",
      "Epoch 17/50: 100% 1328/1328 [00:39<00:00, 33.25it/s, loss=0.000697]\n",
      "Epoch 18/50: 100% 1328/1328 [00:39<00:00, 33.38it/s, loss=0.015502]\n",
      "Epoch 19/50: 100% 1328/1328 [00:39<00:00, 33.89it/s, loss=0.017412]\n",
      "Epoch 20/50: 100% 1328/1328 [00:39<00:00, 33.69it/s, loss=0.003635]\n",
      "Epoch 21/50: 100% 1328/1328 [00:38<00:00, 34.22it/s, loss=0.002366]\n",
      "Epoch 22/50: 100% 1328/1328 [00:38<00:00, 34.12it/s, loss=0.002722]\n",
      "Epoch 23/50: 100% 1328/1328 [00:38<00:00, 34.15it/s, loss=0.011412]\n",
      "Epoch 24/50: 100% 1328/1328 [00:38<00:00, 34.48it/s, loss=0.008910]\n",
      "Epoch 25/50: 100% 1328/1328 [00:37<00:00, 35.50it/s, loss=0.025632]\n",
      "Epoch 26/50: 100% 1328/1328 [00:38<00:00, 34.24it/s, loss=0.009416]\n",
      "Epoch 27/50: 100% 1328/1328 [00:38<00:00, 34.15it/s, loss=0.002567]\n",
      "Epoch 28/50: 100% 1328/1328 [00:39<00:00, 33.99it/s, loss=0.033951]\n",
      "Epoch 29/50: 100% 1328/1328 [00:39<00:00, 33.89it/s, loss=0.016591]\n",
      "Epoch 30/50: 100% 1328/1328 [00:39<00:00, 33.97it/s, loss=0.002142]\n",
      "Epoch 31/50: 100% 1328/1328 [00:38<00:00, 34.18it/s, loss=0.007723]\n",
      "Epoch 32/50: 100% 1328/1328 [00:39<00:00, 34.04it/s, loss=0.004401]\n",
      "Epoch 33/50: 100% 1328/1328 [00:38<00:00, 34.50it/s, loss=0.007649]\n",
      "Epoch 34/50: 100% 1328/1328 [00:39<00:00, 33.33it/s, loss=0.011812]\n",
      "Epoch 35/50: 100% 1328/1328 [00:39<00:00, 33.31it/s, loss=0.030911]\n",
      "Epoch 36/50: 100% 1328/1328 [00:39<00:00, 33.29it/s, loss=0.024033]\n",
      "Epoch 37/50: 100% 1328/1328 [00:39<00:00, 33.43it/s, loss=0.000198]\n",
      "Epoch 38/50: 100% 1328/1328 [00:39<00:00, 33.77it/s, loss=0.003030]\n",
      "Epoch 39/50: 100% 1328/1328 [00:38<00:00, 34.44it/s, loss=0.009571]\n",
      "Epoch 40/50: 100% 1328/1328 [00:39<00:00, 33.88it/s, loss=0.007451]\n",
      "Epoch 41/50: 100% 1328/1328 [00:38<00:00, 34.27it/s, loss=0.010401]\n",
      "Epoch 42/50: 100% 1328/1328 [00:39<00:00, 33.64it/s, loss=0.010320]\n",
      "Epoch 43/50: 100% 1328/1328 [00:39<00:00, 33.68it/s, loss=0.000147]\n",
      "Epoch 44/50: 100% 1328/1328 [00:39<00:00, 33.76it/s, loss=0.007932]\n",
      "Epoch 45/50: 100% 1328/1328 [00:39<00:00, 33.89it/s, loss=0.009709]\n",
      "Epoch 46/50: 100% 1328/1328 [00:38<00:00, 34.24it/s, loss=0.006918]\n",
      "Epoch 47/50: 100% 1328/1328 [00:39<00:00, 33.72it/s, loss=0.016415]\n",
      "Epoch 48/50: 100% 1328/1328 [00:39<00:00, 33.87it/s, loss=0.008011]\n",
      "Epoch 49/50: 100% 1328/1328 [00:39<00:00, 33.87it/s, loss=0.007161]\n",
      "Epoch 50/50: 100% 1328/1328 [00:38<00:00, 34.87it/s, loss=0.006452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LapSRN training complete (best loss: 0.010331)\n",
      "\n",
      "================================================================================\n",
      "[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Dense Blocks\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100% 1328/1328 [01:03<00:00, 20.81it/s, loss=0.003032]\n",
      "Epoch 2/50: 100% 1328/1328 [00:59<00:00, 22.46it/s, loss=0.004422]\n",
      "Epoch 3/50: 100% 1328/1328 [00:59<00:00, 22.27it/s, loss=0.002460]\n",
      "Epoch 4/50: 100% 1328/1328 [00:59<00:00, 22.50it/s, loss=0.002153]\n",
      "Epoch 5/50: 100% 1328/1328 [01:09<00:00, 19.13it/s, loss=0.002845]\n",
      "Epoch 6/50: 100% 1328/1328 [01:19<00:00, 16.69it/s, loss=0.003618]\n",
      "Epoch 7/50: 100% 1328/1328 [01:20<00:00, 16.51it/s, loss=0.002769]\n",
      "Epoch 8/50: 100% 1328/1328 [01:27<00:00, 15.21it/s, loss=0.003031]\n",
      "Epoch 9/50: 100% 1328/1328 [01:32<00:00, 14.34it/s, loss=0.002098]\n",
      "Epoch 10/50: 100% 1328/1328 [01:32<00:00, 14.30it/s, loss=0.000063]\n",
      "Epoch 11/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.004511]\n",
      "Epoch 12/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.002387]\n",
      "Epoch 13/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.008759]\n",
      "Epoch 14/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.001149]\n",
      "Epoch 15/50: 100% 1328/1328 [01:32<00:00, 14.31it/s, loss=0.008853]\n",
      "Epoch 16/50: 100% 1328/1328 [01:32<00:00, 14.29it/s, loss=0.001903]\n",
      "Epoch 17/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.000051]\n",
      "Epoch 18/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.004799]\n",
      "Epoch 19/50: 100% 1328/1328 [01:33<00:00, 14.25it/s, loss=0.000459]\n",
      "Epoch 20/50: 100% 1328/1328 [01:33<00:00, 14.27it/s, loss=0.002311]\n",
      "Epoch 21/50: 100% 1328/1328 [01:32<00:00, 14.31it/s, loss=0.003445]\n",
      "Epoch 22/50: 100% 1328/1328 [01:32<00:00, 14.30it/s, loss=0.000986]\n",
      "Epoch 23/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.002937]\n",
      "Epoch 24/50: 100% 1328/1328 [01:33<00:00, 14.27it/s, loss=0.000039]\n",
      "Epoch 25/50: 100% 1328/1328 [01:32<00:00, 14.34it/s, loss=0.003165]\n",
      "Epoch 26/50: 100% 1328/1328 [01:32<00:00, 14.34it/s, loss=0.005574]\n",
      "Epoch 27/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.003898]\n",
      "Epoch 28/50: 100% 1328/1328 [01:32<00:00, 14.29it/s, loss=0.001329]\n",
      "Epoch 29/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.003442]\n",
      "Epoch 30/50: 100% 1328/1328 [01:32<00:00, 14.29it/s, loss=0.002463]\n",
      "Epoch 31/50: 100% 1328/1328 [01:32<00:00, 14.31it/s, loss=0.002773]\n",
      "Epoch 32/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.007795]\n",
      "Epoch 33/50: 100% 1328/1328 [01:32<00:00, 14.34it/s, loss=0.004772]\n",
      "Epoch 34/50: 100% 1328/1328 [01:32<00:00, 14.34it/s, loss=0.006382]\n",
      "Epoch 35/50: 100% 1328/1328 [01:32<00:00, 14.30it/s, loss=0.000182]\n",
      "Epoch 36/50: 100% 1328/1328 [01:32<00:00, 14.31it/s, loss=0.005224]\n",
      "Epoch 37/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.003981]\n",
      "Epoch 38/50: 100% 1328/1328 [01:32<00:00, 14.30it/s, loss=0.000954]\n",
      "Epoch 39/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.001758]\n",
      "Epoch 40/50: 100% 1328/1328 [01:32<00:00, 14.29it/s, loss=0.002054]\n",
      "Epoch 41/50: 100% 1328/1328 [01:32<00:00, 14.28it/s, loss=0.006552]\n",
      "Epoch 42/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.001602]\n",
      "Epoch 43/50: 100% 1328/1328 [01:32<00:00, 14.35it/s, loss=0.000499]\n",
      "Epoch 44/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.000723]\n",
      "Epoch 45/50: 100% 1328/1328 [01:32<00:00, 14.32it/s, loss=0.002064]\n",
      "Epoch 46/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.004786]\n",
      "Epoch 47/50: 100% 1328/1328 [01:32<00:00, 14.36it/s, loss=0.001203]\n",
      "Epoch 48/50: 100% 1328/1328 [01:32<00:00, 14.30it/s, loss=0.002983]\n",
      "Epoch 49/50: 100% 1328/1328 [01:32<00:00, 14.29it/s, loss=0.006530]\n",
      "Epoch 50/50: 100% 1328/1328 [01:32<00:00, 14.33it/s, loss=0.001765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DRRN training complete (best loss: 0.002348)\n",
      "\n",
      "================================================================================\n",
      "[3/3] Training Classifier (128x128 → 224x224 → Classification)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0% 0/332 [00:00<?, ?it/s]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x16x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x16x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 331/332 [00:34<00:00, 16.83it/s, acc=78.51%]MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x2048x7x7x1x1x1x1x1024x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x7x7x1x3x3x1x512x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x1024x14x14x1x1x1x1x512x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x256x14x14x1x3x3x1x256x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x512x28x28x1x1x1x1x256x12x0x0x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicBwdXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvBinWinogradRxSf3x2; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupBwdXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xB\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvAsmImplicitGemmGTCDynamicWrwXdlopsNHWC; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "MIOpen(HIP): Error [ParseContents] Duplicate ID (ignored): ConvHipImplicitGemmGroupWrwXdlops; key: 2x128x28x28x1x3x3x1x128x12x1x1x0x2x2x0x1x1x0x0x1xNCHWxFP32xW\n",
      "Epoch 1/30: 100% 332/332 [00:38<00:00,  8.71it/s, acc=78.52%]\n",
      "Epoch 2/30: 100% 332/332 [00:20<00:00, 16.20it/s, acc=86.96%]\n",
      "Epoch 3/30: 100% 332/332 [00:20<00:00, 16.22it/s, acc=90.90%]\n",
      "Epoch 4/30: 100% 332/332 [00:20<00:00, 16.37it/s, acc=93.29%]\n",
      "Epoch 5/30: 100% 332/332 [00:20<00:00, 16.16it/s, acc=94.50%]\n",
      "Epoch 6/30: 100% 332/332 [00:20<00:00, 16.10it/s, acc=95.65%]\n",
      "Epoch 7/30: 100% 332/332 [00:20<00:00, 16.11it/s, acc=96.18%]\n",
      "Epoch 8/30: 100% 332/332 [00:20<00:00, 16.52it/s, acc=96.10%]\n",
      "Epoch 9/30: 100% 332/332 [00:20<00:00, 16.21it/s, acc=96.27%]\n",
      "Epoch 10/30: 100% 332/332 [00:21<00:00, 15.50it/s, acc=96.95%]\n",
      "Epoch 11/30: 100% 332/332 [00:21<00:00, 15.46it/s, acc=97.06%]\n",
      "Epoch 12/30: 100% 332/332 [00:21<00:00, 15.76it/s, acc=97.17%]\n",
      "Epoch 13/30: 100% 332/332 [00:21<00:00, 15.46it/s, acc=96.61%]\n",
      "Epoch 14/30: 100% 332/332 [00:21<00:00, 15.43it/s, acc=96.97%]\n",
      "Epoch 15/30: 100% 332/332 [00:21<00:00, 15.45it/s, acc=97.17%]\n",
      "Epoch 16/30: 100% 332/332 [00:21<00:00, 15.78it/s, acc=97.21%]\n",
      "Epoch 17/30: 100% 332/332 [00:21<00:00, 15.65it/s, acc=97.40%]\n",
      "Epoch 18/30: 100% 332/332 [00:21<00:00, 15.47it/s, acc=97.78%]\n",
      "Epoch 19/30: 100% 332/332 [00:21<00:00, 15.67it/s, acc=97.53%]\n",
      "Epoch 20/30: 100% 332/332 [00:21<00:00, 15.68it/s, acc=97.66%]\n",
      "Epoch 21/30: 100% 332/332 [00:21<00:00, 15.44it/s, acc=97.16%]\n",
      "Epoch 22/30: 100% 332/332 [00:21<00:00, 15.55it/s, acc=97.44%]\n",
      "Epoch 23/30: 100% 332/332 [00:21<00:00, 15.59it/s, acc=97.93%]\n",
      "Epoch 24/30: 100% 332/332 [00:21<00:00, 15.80it/s, acc=97.68%]\n",
      "Epoch 25/30: 100% 332/332 [00:21<00:00, 15.52it/s, acc=97.40%]\n",
      "Epoch 26/30: 100% 332/332 [00:21<00:00, 15.49it/s, acc=97.83%]\n",
      "Epoch 27/30: 100% 332/332 [00:21<00:00, 15.60it/s, acc=97.74%]\n",
      "Epoch 28/30: 100% 332/332 [00:18<00:00, 17.61it/s, acc=97.04%]\n",
      "Epoch 29/30: 100% 332/332 [00:16<00:00, 20.70it/s, acc=98.10%]\n",
      "Epoch 30/30: 100% 332/332 [00:18<00:00, 17.54it/s, acc=98.04%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classifier training complete (best accuracy: 98.10%)\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Models saved to: ./trained_models_v9/v9_dense_connections\n",
      "\n",
      "Pipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\n",
      "\n",
      "Key difference from v1_baseline:\n",
      "  - Replaced residual blocks with DenseBlocks in LapSRN\n",
      "  - Added DenseBlocks at DRRN multi-scale collection points\n",
      "  - Each conv layer receives ALL previous layer outputs as input\n",
      "  - 1x1 conv compresses concatenated features back to original channels\n",
      "  - Expected: Better feature reuse, richer representations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Medical Image Training - v9_dense_connections (Dense Connections)\n",
    "Configuration: 64 LapSRN channels, 5 blocks | 128 DRRN channels, 25 blocks | LeakyReLU | Dense Connections\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    VERSION = 'v9_dense_connections'\n",
    "    DATA_DIR = './preprocessed_data'\n",
    "    SAVE_DIR = './trained_models_v9'\n",
    "    \n",
    "    EPOCHS_SR = 50\n",
    "    EPOCHS_CLASS = 30\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    LAPSRN_SCALE = 4\n",
    "    DRRN_SCALE = 2\n",
    "    TOTAL_SCALE = 8\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # v9_dense_connections specific - ADDING DENSE CONNECTIONS\n",
    "    LAPSRN_CHANNELS = 64\n",
    "    LAPSRN_BLOCKS = 5\n",
    "    DRRN_CHANNELS = 128\n",
    "    DRRN_BLOCKS = 25\n",
    "    KERNEL_SIZE = 3\n",
    "    ACTIVATION = 'leaky'\n",
    "    BACKBONE = 'resnet50'\n",
    "    USE_DENSE = True  # NEW: Enable dense connections\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 4\n",
    "        img_path = self.image_files[img_idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = np.random.randint(0, max(1, h - self.hr_patch_size + 1))\n",
    "        left = np.random.randint(0, max(1, w - self.hr_patch_size + 1))\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BUILDING BLOCKS WITH DENSE CONNECTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense block - each conv receives all previous outputs as input\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, num_layers=4, growth_rate=16):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = channels + i * growth_rate\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, growth_rate, kernel_size, padding=padding),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ))\n",
    "        \n",
    "        # 1x1 conv to compress back to original channel count\n",
    "        total_channels = channels + num_layers * growth_rate\n",
    "        self.compress = nn.Conv2d(total_channels, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            concat = torch.cat(features, dim=1)\n",
    "            out = layer(concat)\n",
    "            features.append(out)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        # Compress back to original channels\n",
    "        out = self.compress(dense_out)\n",
    "        # Global residual\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Standard residual block (used outside dense blocks)\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"Standard recursive block (used outside dense blocks)\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODELS - v9_dense_connections\n",
    "# ==============================================================================\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    \"\"\"v9_dense_connections: LapSRN with Dense Blocks replacing residual blocks\"\"\"\n",
    "    def __init__(self, scale_factor=4, num_channels=1, use_dense=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2  # 2x2 = 4x\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            if use_dense:\n",
    "                # Use dense blocks (each dense block replaces multiple residual blocks)\n",
    "                layers.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=16))\n",
    "                layers.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=16))\n",
    "            else:\n",
    "                for _ in range(5):\n",
    "                    layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    \"\"\"v9_dense_connections: DRRN with Dense Blocks at multi-scale collection points\"\"\"\n",
    "    def __init__(self, num_channels=1, scale_factor=2, use_dense=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        # Standard recursive blocks\n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        # Dense blocks placed at collection points for richer feature fusion\n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        if use_dense:\n",
    "            for _ in range(3):  # one per collection point\n",
    "                self.dense_blocks.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=32))\n",
    "        \n",
    "        self.use_dense = use_dense\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        dense_idx = 0\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                # Apply dense block before collecting the feature\n",
    "                if self.use_dense:\n",
    "                    current = self.dense_blocks[dense_idx](current)\n",
    "                    dense_idx += 1\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    \"\"\"v9_dense_connections: Standard ResNet50 classifier (same as baseline)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model():\n",
    "    config = Config()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {config.VERSION.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - LapSRN: {config.LAPSRN_CHANNELS} channels, {config.LAPSRN_BLOCKS} blocks\")\n",
    "    print(f\"  - DRRN: {config.DRRN_CHANNELS} channels, {config.DRRN_BLOCKS} blocks\")\n",
    "    print(f\"  - Kernel: {config.KERNEL_SIZE}x{config.KERNEL_SIZE}\")\n",
    "    print(f\"  - Activation: {config.ACTIVATION}\")\n",
    "    print(f\"  - Backbone: {config.BACKBONE.upper()}\")\n",
    "    print(f\"  - Dense Connections: Enabled\")\n",
    "    print(f\"  - Device: {config.DEVICE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_save_dir = os.path.join(config.SAVE_DIR, config.VERSION)\n",
    "    os.makedirs(version_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    lapsrn = LapSRN(use_dense=config.USE_DENSE).to(config.DEVICE)\n",
    "    drrn = DRRN(use_dense=config.USE_DENSE).to(config.DEVICE)\n",
    "    classifier = MedicalImageClassifier().to(config.DEVICE)\n",
    "    \n",
    "    print(f\"\\nDense Connection info:\")\n",
    "    print(f\"  - Type: DenseBlock (DenseNet-style connections)\")\n",
    "    print(f\"  - LapSRN: 2 DenseBlocks per pyramid level (growth_rate=16, num_layers=4)\")\n",
    "    print(f\"  - DRRN: 3 DenseBlocks at multi-scale collection points (growth_rate=32, num_layers=4)\")\n",
    "    print(f\"  - Each layer receives ALL previous layer outputs as input\")\n",
    "    print(f\"  - 1x1 conv compresses concatenated features back to original channels\")\n",
    "    \n",
    "    # Create datasets\n",
    "    sr_dataset = SuperResolutionDataset(config.DATA_DIR, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(config.DATA_DIR, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(config.DATA_DIR, enhance_size=224)\n",
    "    \n",
    "    # Split datasets (80/20)\n",
    "    train_sr, val_sr = torch.utils.data.random_split(sr_dataset, \n",
    "        [int(0.8*len(sr_dataset)), len(sr_dataset)-int(0.8*len(sr_dataset))])\n",
    "    train_drrn, val_drrn = torch.utils.data.random_split(drrn_dataset,\n",
    "        [int(0.8*len(drrn_dataset)), len(drrn_dataset)-int(0.8*len(drrn_dataset))])\n",
    "    train_class, val_class = torch.utils.data.random_split(class_dataset,\n",
    "        [int(0.8*len(class_dataset)), len(class_dataset)-int(0.8*len(class_dataset))])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sr_loader = DataLoader(train_sr, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_drrn_loader = DataLoader(train_drrn, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    train_class_loader = DataLoader(train_class, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Train LapSRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1/3] Training LapSRN (16x16 → 64x64, 4x upsampling) + Dense Blocks\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(lapsrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.L1Loss()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        lapsrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_sr_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_sr_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(lapsrn.state_dict(), os.path.join(version_save_dir, 'lapsrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ LapSRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train DRRN\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/3] Training DRRN (64x64 → 128x128, 2x upsampling) + Dense Blocks\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(drrn.parameters(), lr=config.LEARNING_RATE)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_SR):\n",
    "        drrn.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_drrn_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_SR}')\n",
    "        \n",
    "        for lr_imgs, hr_imgs in pbar:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(config.DEVICE), hr_imgs.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            loss = criterion(sr_output, hr_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        avg_loss = train_loss / len(train_drrn_loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(drrn.state_dict(), os.path.join(version_save_dir, 'drrn_best.pth'))\n",
    "    \n",
    "    print(f\"✓ DRRN training complete (best loss: {best_loss:.6f})\")\n",
    "    \n",
    "    # Train Classifier\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[3/3] Training Classifier (128x128 → 224x224 → Classification)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.LEARNING_RATE)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    urgency_criterion = nn.BCELoss()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS_CLASS):\n",
    "        classifier.train()\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(train_class_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS_CLASS}')\n",
    "        \n",
    "        for images, labels, urgency in pbar:\n",
    "            images = images.to(config.DEVICE)\n",
    "            labels = labels.to(config.DEVICE)\n",
    "            urgency = urgency.to(config.DEVICE).unsqueeze(1).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            loss = class_criterion(class_out, labels) + 0.5 * urgency_criterion(urgency_out, urgency)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            pbar.set_postfix({'acc': f'{100*correct/total:.2f}%'})\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(classifier.state_dict(), os.path.join(version_save_dir, 'classifier_best.pth'))\n",
    "    \n",
    "    print(f\"✓ Classifier training complete (best accuracy: {best_acc:.2f}%)\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        'version': config.VERSION,\n",
    "        'lapsrn_channels': config.LAPSRN_CHANNELS,\n",
    "        'lapsrn_blocks': config.LAPSRN_BLOCKS,\n",
    "        'drrn_channels': config.DRRN_CHANNELS,\n",
    "        'drrn_blocks': config.DRRN_BLOCKS,\n",
    "        'kernel_size': config.KERNEL_SIZE,\n",
    "        'activation': config.ACTIVATION,\n",
    "        'backbone': config.BACKBONE,\n",
    "        'use_dense': config.USE_DENSE,\n",
    "        'epochs_sr': config.EPOCHS_SR,\n",
    "        'epochs_class': config.EPOCHS_CLASS,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notes': 'Added DenseNet-style dense connections. Each layer receives all previous outputs. 1x1 conv compresses back to original channels.'\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(version_save_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ ALL TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Models saved to: {version_save_dir}\")\n",
    "    print(\"\\nPipeline: 16x16 → LapSRN(4x) → 64x64 → DRRN(2x) → 128x128 → Classifier(224x224)\")\n",
    "    print(\"\\nKey difference from v1_baseline:\")\n",
    "    print(\"  - Replaced residual blocks with DenseBlocks in LapSRN\")\n",
    "    print(\"  - Added DenseBlocks at DRRN multi-scale collection points\")\n",
    "    print(\"  - Each conv layer receives ALL previous layer outputs as input\")\n",
    "    print(\"  - 1x1 conv compresses concatenated features back to original channels\")\n",
    "    print(\"  - Expected: Better feature reuse, richer representations\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427e879e-5a16-4a27-a706-a364a82210cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script - v9_dense_connections (Notebook-Friendly)\n",
    "Calculates PSNR, SSIM, Accuracy, and Classification Metrics\n",
    "\n",
    "Usage in Jupyter Notebook:\n",
    "    from evaluate_v9_dense_connections import evaluate_model\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        version='v9_dense_connections',\n",
    "        data_dir='./preprocessed_data',\n",
    "        model_dir='./trained_models_v9'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, hr_patch_size=64, scale_factor=4):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.lr_patch_size = hr_patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class DRRNDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, patch_size=64, scale_factor=2):\n",
    "        self.hr_patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.image_files = []\n",
    "        \n",
    "        for category in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_files.append(os.path.join(category_path, filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        h, w = img_array.shape\n",
    "        if h < self.hr_patch_size or w < self.hr_patch_size:\n",
    "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            img = img.resize((self.hr_patch_size, self.hr_patch_size), Image.BICUBIC)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            h, w = img_array.shape\n",
    "        \n",
    "        top = (h - self.hr_patch_size) // 2\n",
    "        left = (w - self.hr_patch_size) // 2\n",
    "        hr_patch = img_array[top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        \n",
    "        hr_pil = Image.fromarray((hr_patch * 255).astype(np.uint8))\n",
    "        lr_pil = hr_pil.resize((self.lr_patch_size, self.lr_patch_size), Image.BICUBIC)\n",
    "        lr_patch = np.array(lr_pil, dtype=np.float32) / 255.0\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).unsqueeze(0).float()\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, preprocessed_data_dir, enhance_size=224):\n",
    "        self.enhance_size = enhance_size\n",
    "        self.data = []\n",
    "        \n",
    "        category_map = {'Normal': 0, 'Ischemia': 1, 'Bleeding': 2}\n",
    "        urgency_map = {'Normal': 0.1, 'Ischemia': 0.7, 'Bleeding': 0.95}\n",
    "        \n",
    "        for category, label in category_map.items():\n",
    "            category_path = os.path.join(preprocessed_data_dir, category, '6_Final_Stripped')\n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.data.append({\n",
    "                            'path': os.path.join(category_path, filename),\n",
    "                            'label': label,\n",
    "                            'urgency': urgency_map[category]\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img = Image.open(sample['path']).convert('L')\n",
    "        img = img.resize((self.enhance_size, self.enhance_size), Image.BICUBIC)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return img_tensor, sample['label'], sample['urgency']\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL DEFINITIONS - v9_dense_connections\n",
    "# ==============================================================================\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, num_layers=4, growth_rate=16):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = channels + i * growth_rate\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, growth_rate, kernel_size, padding=padding),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ))\n",
    "        \n",
    "        total_channels = channels + num_layers * growth_rate\n",
    "        self.compress = nn.Conv2d(total_channels, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            concat = torch.cat(features, dim=1)\n",
    "            out = layer(concat)\n",
    "            features.append(out)\n",
    "        \n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        out = self.compress(dense_out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=padding)\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class LapSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=4, num_channels=1, use_dense=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_levels = 2\n",
    "        ch = 64\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.pyramid_levels = nn.ModuleList()\n",
    "        self.image_reconstruction = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.num_levels):\n",
    "            layers = []\n",
    "            if use_dense:\n",
    "                layers.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=16))\n",
    "                layers.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=16))\n",
    "            else:\n",
    "                for _ in range(5):\n",
    "                    layers.append(ResidualBlock(ch, 3))\n",
    "            layers.append(nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            \n",
    "            self.pyramid_levels.append(nn.Sequential(*layers))\n",
    "            self.image_reconstruction.append(nn.Conv2d(ch, num_channels, 3, padding=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        outputs = []\n",
    "        current_features = features\n",
    "        \n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_features = self.pyramid_levels[level_idx](current_features)\n",
    "            img_out = self.image_reconstruction[level_idx](current_features)\n",
    "            \n",
    "            if level_idx > 0:\n",
    "                img_out = img_out + F.interpolate(outputs[-1], scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                img_out = img_out + F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(img_out)\n",
    "        \n",
    "        return outputs[-1], outputs\n",
    "\n",
    "\n",
    "class DRRN(nn.Module):\n",
    "    def __init__(self, num_channels=1, scale_factor=2, use_dense=True):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        ch = 128\n",
    "        \n",
    "        self.input_conv = nn.Conv2d(num_channels, ch, 3, padding=1)\n",
    "        \n",
    "        self.recursive_blocks = nn.ModuleList()\n",
    "        for _ in range(25):\n",
    "            self.recursive_blocks.append(RecursiveBlock(ch, 3))\n",
    "        \n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        if use_dense:\n",
    "            for _ in range(3):\n",
    "                self.dense_blocks.append(DenseBlock(ch, kernel_size=3, num_layers=4, growth_rate=32))\n",
    "        \n",
    "        self.use_dense = use_dense\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(ch * 3, ch, 1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch * 4, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )\n",
    "        \n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, num_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode='bicubic', align_corners=False)\n",
    "        \n",
    "        features = self.input_conv(x)\n",
    "        multi_scale_features = []\n",
    "        current = features\n",
    "        \n",
    "        collect_indices = [8, 16, 24]\n",
    "        dense_idx = 0\n",
    "        \n",
    "        for idx, block in enumerate(self.recursive_blocks):\n",
    "            current = block(current)\n",
    "            if idx in collect_indices:\n",
    "                if self.use_dense:\n",
    "                    current = self.dense_blocks[dense_idx](current)\n",
    "                    dense_idx += 1\n",
    "                multi_scale_features.append(current)\n",
    "        \n",
    "        fused = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        upsampled = self.upsample(fused)\n",
    "        output = self.output_conv(upsampled)\n",
    "        \n",
    "        return output + input_upsampled\n",
    "\n",
    "\n",
    "class MedicalImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from torchvision import models\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.urgency_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.feature_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classification_head(features), self.urgency_head(features), self.feature_head(features)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(version='v9_dense_connections', data_dir='./preprocessed_data', \n",
    "                   model_dir='./trained_models_v9',\n",
    "                   device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING MODEL: {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    version_dir = os.path.join(model_dir, version)\n",
    "    config_path = os.path.join(version_dir, 'config.json')\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"ERROR: Config file not found at {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded configuration for {version}\")\n",
    "    print(f\"  LapSRN: {config.get('lapsrn_channels', 64)} channels + DenseBlocks (growth_rate=16)\")\n",
    "    print(f\"  DRRN: {config.get('drrn_channels', 128)} channels + DenseBlocks (growth_rate=32)\")\n",
    "    print(f\"  Backbone: {config.get('backbone', 'resnet50')}\")\n",
    "    print(f\"  Dense Connections: {config.get('use_dense', True)}\\n\")\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    sr_dataset = SuperResolutionDataset(data_dir, hr_patch_size=64, scale_factor=4)\n",
    "    drrn_dataset = DRRNDataset(data_dir, patch_size=64, scale_factor=2)\n",
    "    class_dataset = ClassificationDataset(data_dir, enhance_size=224)\n",
    "    \n",
    "    sr_loader = DataLoader(sr_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    drrn_loader = DataLoader(drrn_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"  SR dataset: {len(sr_dataset)} samples\")\n",
    "    print(f\"  DRRN dataset: {len(drrn_dataset)} samples\")\n",
    "    print(f\"  Classification dataset: {len(class_dataset)} samples\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    lapsrn = LapSRN(use_dense=True).to(device)\n",
    "    drrn = DRRN(use_dense=True).to(device)\n",
    "    classifier = MedicalImageClassifier().to(device)\n",
    "    \n",
    "    lapsrn.load_state_dict(torch.load(os.path.join(version_dir, 'lapsrn_best.pth'), map_location=device))\n",
    "    drrn.load_state_dict(torch.load(os.path.join(version_dir, 'drrn_best.pth'), map_location=device))\n",
    "    classifier.load_state_dict(torch.load(os.path.join(version_dir, 'classifier_best.pth'), map_location=device))\n",
    "    \n",
    "    lapsrn.eval()\n",
    "    drrn.eval()\n",
    "    classifier.eval()\n",
    "    print(\"✓ Models loaded successfully\\n\")\n",
    "    \n",
    "    # Evaluate LapSRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[1/3] Evaluating LapSRN (16x16 → 64x64)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    lapsrn_psnr_list, lapsrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(sr_loader, desc=\"LapSRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output, _ = lapsrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                lapsrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                lapsrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    lapsrn_psnr_mean = np.mean(lapsrn_psnr_list)\n",
    "    lapsrn_ssim_mean = np.mean(lapsrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ LapSRN Results:\")\n",
    "    print(f\"  PSNR: {lapsrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {lapsrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate DRRN\n",
    "    print(\"=\"*80)\n",
    "    print(\"[2/3] Evaluating DRRN (64x64 → 128x128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    drrn_psnr_list, drrn_ssim_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(drrn_loader, desc=\"DRRN Evaluation\"):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            sr_output = drrn(lr_imgs)\n",
    "            \n",
    "            sr_np = sr_output.cpu().numpy()\n",
    "            hr_np = hr_imgs.numpy()\n",
    "            \n",
    "            for i in range(sr_np.shape[0]):\n",
    "                sr_img = np.clip(sr_np[i, 0], 0, 1)\n",
    "                hr_img = np.clip(hr_np[i, 0], 0, 1)\n",
    "                drrn_psnr_list.append(psnr(hr_img, sr_img, data_range=1.0))\n",
    "                drrn_ssim_list.append(ssim(hr_img, sr_img, data_range=1.0))\n",
    "    \n",
    "    drrn_psnr_mean = np.mean(drrn_psnr_list)\n",
    "    drrn_ssim_mean = np.mean(drrn_ssim_list)\n",
    "    \n",
    "    print(f\"\\n✓ DRRN Results:\")\n",
    "    print(f\"  PSNR: {drrn_psnr_mean:.4f} dB\")\n",
    "    print(f\"  SSIM: {drrn_ssim_mean:.4f}\\n\")\n",
    "    \n",
    "    # Evaluate Classifier\n",
    "    print(\"=\"*80)\n",
    "    print(\"[3/3] Evaluating Classifier\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_preds, all_labels, all_urgency_preds, all_urgency_true = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, urgency in tqdm(class_loader, desc=\"Classifier Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            class_out, urgency_out, _ = classifier(images)\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_urgency_preds.extend(urgency_out.cpu().numpy().flatten())\n",
    "            all_urgency_true.extend(urgency.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                        target_names=['Normal', 'Ischemia', 'Bleeding'], output_dict=True)\n",
    "    urgency_mse = np.mean((np.array(all_urgency_preds) - np.array(all_urgency_true))**2)\n",
    "    urgency_mae = np.mean(np.abs(np.array(all_urgency_preds) - np.array(all_urgency_true)))\n",
    "    \n",
    "    print(f\"\\n✓ Classification Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  {conf_matrix}\")\n",
    "    print(f\"\\n  Per-Class Metrics:\")\n",
    "    for class_name in ['Normal', 'Ischemia', 'Bleeding']:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"    {class_name}:\")\n",
    "        print(f\"      Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"      Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"      F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Urgency Prediction:\")\n",
    "    print(f\"    MSE: {urgency_mse:.4f}\")\n",
    "    print(f\"    MAE: {urgency_mae:.4f}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'version': version,\n",
    "        'lapsrn': { 'psnr': float(lapsrn_psnr_mean), 'ssim': float(lapsrn_ssim_mean) },\n",
    "        'drrn': { 'psnr': float(drrn_psnr_mean), 'ssim': float(drrn_ssim_mean) },\n",
    "        'classifier': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'urgency_mse': float(urgency_mse),\n",
    "            'urgency_mae': float(urgency_mae)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(version_dir, 'evaluation_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {results_path}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION COMPLETE FOR {version}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b533dc4-034c-41ab-a513-908985468085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING MODEL: v9_dense_connections\n",
      "================================================================================\n",
      "\n",
      "Loaded configuration for v9_dense_connections\n",
      "  LapSRN: 64 channels + DenseBlocks (growth_rate=16)\n",
      "  DRRN: 128 channels + DenseBlocks (growth_rate=32)\n",
      "  Backbone: resnet50\n",
      "  Dense Connections: True\n",
      "\n",
      "Creating datasets...\n",
      "  SR dataset: 6636 samples\n",
      "  DRRN dataset: 6636 samples\n",
      "  Classification dataset: 6636 samples\n",
      "\n",
      "Loading models...\n",
      "✓ Models loaded successfully\n",
      "\n",
      "================================================================================\n",
      "[1/3] Evaluating LapSRN (16x16 → 64x64)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LapSRN Evaluation: 100% 415/415 [00:12<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ LapSRN Results:\n",
      "  PSNR: 32.7081 dB\n",
      "  SSIM: 0.8212\n",
      "\n",
      "================================================================================\n",
      "[2/3] Evaluating DRRN (64x64 → 128x128)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DRRN Evaluation: 100% 415/415 [00:12<00:00, 33.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DRRN Results:\n",
      "  PSNR: 45.2190 dB\n",
      "  SSIM: 0.9839\n",
      "\n",
      "================================================================================\n",
      "[3/3] Evaluating Classifier\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Evaluation: 100% 415/415 [00:14<00:00, 28.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification Results:\n",
      "  Accuracy: 97.24%\n",
      "\n",
      "  Confusion Matrix:\n",
      "  [[4402   21    4]\n",
      " [  57 1058    1]\n",
      " [  95    5  993]]\n",
      "\n",
      "  Per-Class Metrics:\n",
      "    Normal:\n",
      "      Precision: 0.9666\n",
      "      Recall: 0.9944\n",
      "      F1-Score: 0.9803\n",
      "    Ischemia:\n",
      "      Precision: 0.9760\n",
      "      Recall: 0.9480\n",
      "      F1-Score: 0.9618\n",
      "    Bleeding:\n",
      "      Precision: 0.9950\n",
      "      Recall: 0.9085\n",
      "      F1-Score: 0.9498\n",
      "\n",
      "  Urgency Prediction:\n",
      "    MSE: 0.0130\n",
      "    MAE: 0.0622\n",
      "\n",
      "✓ Results saved to: ./trained_models_v9/v9_dense_connections/evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE FOR v9_dense_connections\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(version='v9_dense_connections', data_dir='./preprocessed_data', model_dir='./trained_models_v9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb44f2d-4f28-4641-9d97-504efbfa3bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
